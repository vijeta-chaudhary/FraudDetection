{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1MOTElcOUWqeNNn-CymdL2oMgICe9ecN3",
      "authorship_tag": "ABX9TyNhoMVcOxDKSzVyiunOtH9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varad8801/Ml-Projects/blob/main/Credit_Crad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fraudulent credit card transactions among a dataset of European cardholder**"
      ],
      "metadata": {
        "id": "Yps5jdg8QJGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "The accuracy chart compares the performance of various models on a dataset, which comprises over 550,000 records of credit card transactions made by European cardholders in 2023. Credit card fraud is a major concern for financial institutions and cardholders, leading to significant financial losses and security issues. With the increasing volume of digital transactions, traditional rule-based systems are becoming less effective, necessitating the need for advanced, scalable, and efficient fraud detection solutions.\n",
        "\n",
        "The models include Logistic Regression, Decision Trees, Random Forest, SVM, and Gradient Boosting. Two filters are applied for feature selection, addressing multicollinearity and removing less-correlated columns with the target.\n",
        "\n",
        "1. **Logistic Regression:**\n",
        "   - Achieved accuracies of 93.8% and 83.59%.\n",
        "\n",
        "2. **Decision Trees:**\n",
        "   - Achieved accuracies of 98.9% (using entropy criterion) and 95.26%.\n",
        "\n",
        "3. **Random Forest:**\n",
        "   - Achieved an accuracy of 97.24%.\n",
        "\n",
        "4. **Support Vector Machine (SVM):**\n",
        "   - Achieved an accuracy of 89.14%.\n",
        "\n",
        "5. **Gradient Boosting:**\n",
        "   - Achieved an accuracy of 89.16%.\n",
        "\n",
        "6. **Artificial Neural Network (ANN):**\n",
        "   - Achieved an accuracy of 89.22%.\n",
        "\n",
        "7. **Random Forest Hyperparameter Tuning(Random Search CV):**\n",
        "   - A Random Forest model was trained on a subsample of the data (5,000 records) for efficient hyperparameter tuning.\n",
        "   - Hyperparameters were tuned using RandomizedSearchCV, exploring various combinations within defined ranges.\n",
        "   - The best hyperparameters found:\n",
        "      - n_estimators: 150\n",
        "      - max_depth: 20\n",
        "      - min_samples_split: 5\n",
        "      - min_samples_leaf: 2\n",
        "   - The tuned Random Forest model achieved an accuracy of 89.98% on the test set.\n",
        "\n",
        "\n",
        "8. **Random Forest Hyperparameter Tuning (GridSearchCV):**\n",
        "   - A Random Forest model was trained on a subsample of the data (5,000 records) for efficient hyperparameter tuning.\n",
        "   - Hyperparameters were tuned using GridSearchCV, exhaustively searching through predefined combinations within specified ranges.\n",
        "   - The best hyperparameters found:\n",
        "      - n_estimators: 150\n",
        "      - max_depth: 20\n",
        "      - min_samples_split: 2\n",
        "      - min_samples_leaf: 4\n",
        "   - The tuned Random Forest model achieved an accuracy of 90.35% on the test set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Feature Selection:**\n",
        "   - Two filters, `Filter1` and `Filter2`, are applied.\n",
        "   - `Filter1` removes multicollinearity.\n",
        "   - `Filter2` removes less-correlated columns with values.\n",
        "**Data Balancing:**\n",
        "   - The dataset is balanced, as confirmed by the count plot.\n",
        "   - Balanced data ensures that the models are not biased towards the majority class, improving overall performance and generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRJy8vW_cEpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#feature selection"
      ],
      "metadata": {
        "id": "wiFjXKxSJCvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Finding the correlation"
      ],
      "metadata": {
        "id": "4qLjqQb7CXhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0q5UJX9vTF05",
        "outputId": "01e428f5-2ce7-4249-f0aa-c6d7c3756741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              id        V1        V2        V3        V4        V5        V6  \\\n",
              "id      1.000000 -0.395741  0.424267 -0.663655  0.617554 -0.268445 -0.387916   \n",
              "V1     -0.395741  1.000000 -0.561184  0.484499 -0.498963  0.517462  0.354728   \n",
              "V2      0.424267 -0.561184  1.000000 -0.627810  0.579638 -0.631669 -0.341040   \n",
              "V3     -0.663655  0.484499 -0.627810  1.000000 -0.687726  0.510351  0.508974   \n",
              "V4      0.617554 -0.498963  0.579638 -0.687726  1.000000 -0.429243 -0.474403   \n",
              "V5     -0.268445  0.517462 -0.631669  0.510351 -0.429243  1.000000  0.245187   \n",
              "V6     -0.387916  0.354728 -0.341040  0.508974 -0.474403  0.245187  1.000000   \n",
              "V7     -0.414288  0.573381 -0.694022  0.634336 -0.588648  0.586828  0.418703   \n",
              "V8      0.121282 -0.226757  0.191321 -0.263018  0.199013 -0.314975 -0.604491   \n",
              "V9     -0.508427  0.548973 -0.585095  0.648615 -0.676648  0.479614  0.432241   \n",
              "V10    -0.578014  0.599108 -0.621798  0.707676 -0.712839  0.563874  0.471000   \n",
              "V11     0.589321 -0.525797  0.558863 -0.688436  0.708642 -0.440100 -0.497611   \n",
              "V12    -0.652940  0.580715 -0.574935  0.705497 -0.722597  0.473002  0.498993   \n",
              "V13    -0.076331 -0.020567  0.012801 -0.019272  0.011519 -0.115317 -0.117637   \n",
              "V14    -0.709346  0.494427 -0.523294  0.673179 -0.714847  0.387454  0.510123   \n",
              "V15    -0.080004  0.046002 -0.161325  0.098516 -0.098627  0.058686 -0.023851   \n",
              "V16    -0.494255  0.621884 -0.534392  0.614504 -0.593948  0.596898  0.415834   \n",
              "V17    -0.417226  0.605799 -0.495836  0.578223 -0.532786  0.669625  0.378152   \n",
              "V18    -0.341056  0.577296 -0.482162  0.525509 -0.482267  0.645095  0.328019   \n",
              "V19     0.216276 -0.377803  0.208821 -0.314396  0.269842 -0.438118 -0.235623   \n",
              "V20     0.145803 -0.219164  0.263707 -0.253805  0.257236 -0.246694 -0.188360   \n",
              "V21     0.097948 -0.034669 -0.013570 -0.021710 -0.013093  0.034147 -0.040153   \n",
              "V22     0.036106 -0.073729  0.035346 -0.041970  0.091197 -0.119152  0.036896   \n",
              "V23     0.017594 -0.068917  0.151906 -0.058884  0.043266 -0.113919  0.308598   \n",
              "V24    -0.116685 -0.014651 -0.027515  0.076460 -0.102508 -0.083243 -0.005237   \n",
              "V25     0.005586 -0.008508  0.132443 -0.076332  0.029402 -0.047845 -0.195340   \n",
              "V26     0.052126  0.009281  0.012219 -0.052056  0.136679  0.047771 -0.067605   \n",
              "V27     0.184195 -0.122772  0.053835 -0.190582  0.188036 -0.043759 -0.260783   \n",
              "V28     0.086822  0.070111  0.021071  0.005346 -0.011316  0.108422 -0.065641   \n",
              "Amount  0.001710 -0.001280 -0.000076 -0.002001  0.001859 -0.000016  0.000734   \n",
              "Class   0.864283 -0.505761  0.491878 -0.682095  0.735981 -0.338639 -0.435088   \n",
              "\n",
              "              V7        V8        V9       V10       V11       V12       V13  \\\n",
              "id     -0.414288  0.121282 -0.508427 -0.578014  0.589321 -0.652940 -0.076331   \n",
              "V1      0.573381 -0.226757  0.548973  0.599108 -0.525797  0.580715 -0.020567   \n",
              "V2     -0.694022  0.191321 -0.585095 -0.621798  0.558863 -0.574935  0.012801   \n",
              "V3      0.634336 -0.263018  0.648615  0.707676 -0.688436  0.705497 -0.019272   \n",
              "V4     -0.588648  0.199013 -0.676648 -0.712839  0.708642 -0.722597  0.011519   \n",
              "V5      0.586828 -0.314975  0.479614  0.563874 -0.440100  0.473002 -0.115317   \n",
              "V6      0.418703 -0.604491  0.432241  0.471000 -0.497611  0.498993 -0.117637   \n",
              "V7      1.000000 -0.180986  0.601789  0.678004 -0.587660  0.603318 -0.030000   \n",
              "V8     -0.180986  1.000000 -0.208557 -0.199995  0.223052 -0.211999  0.273958   \n",
              "V9      0.601789 -0.208557  1.000000  0.748487 -0.633556  0.667266 -0.006167   \n",
              "V10     0.678004 -0.199995  0.748487  1.000000 -0.713066  0.736783 -0.019246   \n",
              "V11    -0.587660  0.223052 -0.633556 -0.713066  1.000000 -0.744642  0.014518   \n",
              "V12     0.603318 -0.211999  0.667266  0.736783 -0.744642  1.000000  0.018336   \n",
              "V13    -0.030000  0.273958 -0.006167 -0.019246  0.014518  0.018336  1.000000   \n",
              "V14     0.535612 -0.216410  0.633212  0.698939 -0.762322  0.783878  0.028738   \n",
              "V15     0.135939  0.101690  0.114613  0.111051 -0.058089  0.041208 -0.016044   \n",
              "V16     0.667244 -0.230638  0.573957  0.686602 -0.655081  0.698490 -0.082089   \n",
              "V17     0.655755 -0.277246  0.581604  0.649149 -0.601924  0.658739 -0.123173   \n",
              "V18     0.625680 -0.249986  0.522720  0.596702 -0.519721  0.579374 -0.122049   \n",
              "V19    -0.372270  0.253272 -0.294432 -0.375080  0.346170 -0.368161  0.167676   \n",
              "V20    -0.299436  0.131354 -0.328975 -0.287051  0.204378 -0.219275 -0.007267   \n",
              "V21     0.019627  0.056416  0.131001  0.037426  0.111608 -0.080394  0.025529   \n",
              "V22    -0.104043 -0.098752 -0.204723 -0.150957  0.022153 -0.072096  0.002039   \n",
              "V23    -0.111177 -0.463649 -0.042371 -0.056285  0.013596 -0.019261 -0.123520   \n",
              "V24    -0.004152  0.083272  0.044006  0.045935 -0.104340  0.080407  0.060097   \n",
              "V25     0.000802  0.322639 -0.034885 -0.014045  0.051535 -0.010350  0.003580   \n",
              "V26    -0.006488  0.040448 -0.131000 -0.053684  0.133635 -0.114272  0.043750   \n",
              "V27    -0.036557  0.298398 -0.111842 -0.134907  0.290912 -0.216563  0.058483   \n",
              "V28     0.040732  0.046017  0.069959  0.035646  0.059732 -0.053136 -0.101488   \n",
              "Amount  0.001326 -0.000208 -0.001589 -0.001259  0.000292 -0.001245 -0.002718   \n",
              "Class  -0.491234  0.144294 -0.585522 -0.673665  0.724278 -0.768579 -0.071105   \n",
              "\n",
              "             V14       V15       V16       V17       V18       V19       V20  \\\n",
              "id     -0.709346 -0.080004 -0.494255 -0.417226 -0.341056  0.216276  0.145803   \n",
              "V1      0.494427  0.046002  0.621884  0.605799  0.577296 -0.377803 -0.219164   \n",
              "V2     -0.523294 -0.161325 -0.534392 -0.495836 -0.482162  0.208821  0.263707   \n",
              "V3      0.673179  0.098516  0.614504  0.578223  0.525509 -0.314396 -0.253805   \n",
              "V4     -0.714847 -0.098627 -0.593948 -0.532786 -0.482267  0.269842  0.257236   \n",
              "V5      0.387454  0.058686  0.596898  0.669625  0.645095 -0.438118 -0.246694   \n",
              "V6      0.510123 -0.023851  0.415834  0.378152  0.328019 -0.235623 -0.188360   \n",
              "V7      0.535612  0.135939  0.667244  0.655755  0.625680 -0.372270 -0.299436   \n",
              "V8     -0.216410  0.101690 -0.230638 -0.277246 -0.249986  0.253272  0.131354   \n",
              "V9      0.633212  0.114613  0.573957  0.581604  0.522720 -0.294432 -0.328975   \n",
              "V10     0.698939  0.111051  0.686602  0.649149  0.596702 -0.375080 -0.287051   \n",
              "V11    -0.762322 -0.058089 -0.655081 -0.601924 -0.519721  0.346170  0.204378   \n",
              "V12     0.783878  0.041208  0.698490  0.658739  0.579374 -0.368161 -0.219275   \n",
              "V13     0.028738 -0.016044 -0.082089 -0.123173 -0.122049  0.167676 -0.007267   \n",
              "V14     1.000000  0.010011  0.630516  0.552428  0.469393 -0.315192 -0.148104   \n",
              "V15     0.010011  1.000000  0.001357  0.031912  0.017136  0.185638 -0.135423   \n",
              "V16     0.630516  0.001357  1.000000  0.848095  0.767992 -0.585409 -0.223431   \n",
              "V17     0.552428  0.031912  0.848095  1.000000  0.851366 -0.614222 -0.215708   \n",
              "V18     0.469393  0.017136  0.767992  0.851366  1.000000 -0.565991 -0.185670   \n",
              "V19    -0.315192  0.185638 -0.585409 -0.614222 -0.565991  1.000000  0.071641   \n",
              "V20    -0.148104 -0.135423 -0.223431 -0.215708 -0.185670  0.071641  1.000000   \n",
              "V21    -0.189902  0.171719 -0.117591 -0.079348 -0.060862  0.136080 -0.529918   \n",
              "V22     0.052023 -0.099347 -0.101847 -0.144637 -0.135994  0.110066  0.429362   \n",
              "V23    -0.007601 -0.074832 -0.057100 -0.044635 -0.046262 -0.001529  0.017204   \n",
              "V24     0.138718  0.023003 -0.023511 -0.072198 -0.099745  0.110751 -0.020316   \n",
              "V25    -0.087040 -0.027579  0.062484  0.075609  0.070467 -0.174328  0.030478   \n",
              "V26    -0.142472  0.047833 -0.056184 -0.045189 -0.021039  0.041421  0.007677   \n",
              "V27    -0.299951  0.116106 -0.191742 -0.184550 -0.141790  0.123266 -0.055183   \n",
              "V28    -0.127969  0.100293 -0.022328  0.019570  0.052547 -0.024368 -0.035727   \n",
              "Amount -0.001363  0.001190 -0.000479 -0.000358 -0.001516 -0.000400 -0.001405   \n",
              "Class  -0.805669 -0.037948 -0.573511 -0.476377 -0.410091  0.244081  0.179851   \n",
              "\n",
              "             V21       V22       V23       V24       V25       V26       V27  \\\n",
              "id      0.097948  0.036106  0.017594 -0.116685  0.005586  0.052126  0.184195   \n",
              "V1     -0.034669 -0.073729 -0.068917 -0.014651 -0.008508  0.009281 -0.122772   \n",
              "V2     -0.013570  0.035346  0.151906 -0.027515  0.132443  0.012219  0.053835   \n",
              "V3     -0.021710 -0.041970 -0.058884  0.076460 -0.076332 -0.052056 -0.190582   \n",
              "V4     -0.013093  0.091197  0.043266 -0.102508  0.029402  0.136679  0.188036   \n",
              "V5      0.034147 -0.119152 -0.113919 -0.083243 -0.047845  0.047771 -0.043759   \n",
              "V6     -0.040153  0.036896  0.308598 -0.005237 -0.195340 -0.067605 -0.260783   \n",
              "V7      0.019627 -0.104043 -0.111177 -0.004152  0.000802 -0.006488 -0.036557   \n",
              "V8      0.056416 -0.098752 -0.463649  0.083272  0.322639  0.040448  0.298398   \n",
              "V9      0.131001 -0.204723 -0.042371  0.044006 -0.034885 -0.131000 -0.111842   \n",
              "V10     0.037426 -0.150957 -0.056285  0.045935 -0.014045 -0.053684 -0.134907   \n",
              "V11     0.111608  0.022153  0.013596 -0.104340  0.051535  0.133635  0.290912   \n",
              "V12    -0.080394 -0.072096 -0.019261  0.080407 -0.010350 -0.114272 -0.216563   \n",
              "V13     0.025529  0.002039 -0.123520  0.060097  0.003580  0.043750  0.058483   \n",
              "V14    -0.189902  0.052023 -0.007601  0.138718 -0.087040 -0.142472 -0.299951   \n",
              "V15     0.171719 -0.099347 -0.074832  0.023003 -0.027579  0.047833  0.116106   \n",
              "V16    -0.117591 -0.101847 -0.057100 -0.023511  0.062484 -0.056184 -0.191742   \n",
              "V17    -0.079348 -0.144637 -0.044635 -0.072198  0.075609 -0.045189 -0.184550   \n",
              "V18    -0.060862 -0.135994 -0.046262 -0.099745  0.070467 -0.021039 -0.141790   \n",
              "V19     0.136080  0.110066 -0.001529  0.110751 -0.174328  0.041421  0.123266   \n",
              "V20    -0.529918  0.429362  0.017204 -0.020316  0.030478  0.007677 -0.055183   \n",
              "V21     1.000000 -0.734653  0.096587 -0.059190  0.146164  0.070050  0.373256   \n",
              "V22    -0.734653  1.000000 -0.000636  0.079790 -0.258956 -0.015127 -0.340640   \n",
              "V23     0.096587 -0.000636  1.000000 -0.051181 -0.040882  0.001057 -0.151698   \n",
              "V24    -0.059190  0.079790 -0.051181  1.000000 -0.079604 -0.113362 -0.194899   \n",
              "V25     0.146164 -0.258956 -0.040882 -0.079604  1.000000  0.057546  0.215653   \n",
              "V26     0.070050 -0.015127  0.001057 -0.113362  0.057546  1.000000  0.193977   \n",
              "V27     0.373256 -0.340640 -0.151698 -0.194899  0.215653  0.193977  1.000000   \n",
              "V28     0.326677 -0.282893  0.028059 -0.045189  0.176058  0.036830  0.183233   \n",
              "Amount  0.001029 -0.000942 -0.001981 -0.000846 -0.000720 -0.000120  0.001235   \n",
              "Class   0.109640  0.014098  0.010255 -0.130107  0.061847  0.071052  0.214002   \n",
              "\n",
              "             V28    Amount     Class  \n",
              "id      0.086822  0.001710  0.864283  \n",
              "V1      0.070111 -0.001280 -0.505761  \n",
              "V2      0.021071 -0.000076  0.491878  \n",
              "V3      0.005346 -0.002001 -0.682095  \n",
              "V4     -0.011316  0.001859  0.735981  \n",
              "V5      0.108422 -0.000016 -0.338639  \n",
              "V6     -0.065641  0.000734 -0.435088  \n",
              "V7      0.040732  0.001326 -0.491234  \n",
              "V8      0.046017 -0.000208  0.144294  \n",
              "V9      0.069959 -0.001589 -0.585522  \n",
              "V10     0.035646 -0.001259 -0.673665  \n",
              "V11     0.059732  0.000292  0.724278  \n",
              "V12    -0.053136 -0.001245 -0.768579  \n",
              "V13    -0.101488 -0.002718 -0.071105  \n",
              "V14    -0.127969 -0.001363 -0.805669  \n",
              "V15     0.100293  0.001190 -0.037948  \n",
              "V16    -0.022328 -0.000479 -0.573511  \n",
              "V17     0.019570 -0.000358 -0.476377  \n",
              "V18     0.052547 -0.001516 -0.410091  \n",
              "V19    -0.024368 -0.000400  0.244081  \n",
              "V20    -0.035727 -0.001405  0.179851  \n",
              "V21     0.326677  0.001029  0.109640  \n",
              "V22    -0.282893 -0.000942  0.014098  \n",
              "V23     0.028059 -0.001981  0.010255  \n",
              "V24    -0.045189 -0.000846 -0.130107  \n",
              "V25     0.176058 -0.000720  0.061847  \n",
              "V26     0.036830 -0.000120  0.071052  \n",
              "V27     0.183233  0.001235  0.214002  \n",
              "V28     1.000000 -0.001503  0.102024  \n",
              "Amount -0.001503  1.000000  0.002261  \n",
              "Class   0.102024  0.002261  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3b48174-9e0d-4cbd-8d75-e838bd5842a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.395741</td>\n",
              "      <td>0.424267</td>\n",
              "      <td>-0.663655</td>\n",
              "      <td>0.617554</td>\n",
              "      <td>-0.268445</td>\n",
              "      <td>-0.387916</td>\n",
              "      <td>-0.414288</td>\n",
              "      <td>0.121282</td>\n",
              "      <td>-0.508427</td>\n",
              "      <td>-0.578014</td>\n",
              "      <td>0.589321</td>\n",
              "      <td>-0.652940</td>\n",
              "      <td>-0.076331</td>\n",
              "      <td>-0.709346</td>\n",
              "      <td>-0.080004</td>\n",
              "      <td>-0.494255</td>\n",
              "      <td>-0.417226</td>\n",
              "      <td>-0.341056</td>\n",
              "      <td>0.216276</td>\n",
              "      <td>0.145803</td>\n",
              "      <td>0.097948</td>\n",
              "      <td>0.036106</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>-0.116685</td>\n",
              "      <td>0.005586</td>\n",
              "      <td>0.052126</td>\n",
              "      <td>0.184195</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.864283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>-0.395741</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.561184</td>\n",
              "      <td>0.484499</td>\n",
              "      <td>-0.498963</td>\n",
              "      <td>0.517462</td>\n",
              "      <td>0.354728</td>\n",
              "      <td>0.573381</td>\n",
              "      <td>-0.226757</td>\n",
              "      <td>0.548973</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>-0.525797</td>\n",
              "      <td>0.580715</td>\n",
              "      <td>-0.020567</td>\n",
              "      <td>0.494427</td>\n",
              "      <td>0.046002</td>\n",
              "      <td>0.621884</td>\n",
              "      <td>0.605799</td>\n",
              "      <td>0.577296</td>\n",
              "      <td>-0.377803</td>\n",
              "      <td>-0.219164</td>\n",
              "      <td>-0.034669</td>\n",
              "      <td>-0.073729</td>\n",
              "      <td>-0.068917</td>\n",
              "      <td>-0.014651</td>\n",
              "      <td>-0.008508</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>-0.122772</td>\n",
              "      <td>0.070111</td>\n",
              "      <td>-0.001280</td>\n",
              "      <td>-0.505761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>0.424267</td>\n",
              "      <td>-0.561184</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.627810</td>\n",
              "      <td>0.579638</td>\n",
              "      <td>-0.631669</td>\n",
              "      <td>-0.341040</td>\n",
              "      <td>-0.694022</td>\n",
              "      <td>0.191321</td>\n",
              "      <td>-0.585095</td>\n",
              "      <td>-0.621798</td>\n",
              "      <td>0.558863</td>\n",
              "      <td>-0.574935</td>\n",
              "      <td>0.012801</td>\n",
              "      <td>-0.523294</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.534392</td>\n",
              "      <td>-0.495836</td>\n",
              "      <td>-0.482162</td>\n",
              "      <td>0.208821</td>\n",
              "      <td>0.263707</td>\n",
              "      <td>-0.013570</td>\n",
              "      <td>0.035346</td>\n",
              "      <td>0.151906</td>\n",
              "      <td>-0.027515</td>\n",
              "      <td>0.132443</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>0.491878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>-0.663655</td>\n",
              "      <td>0.484499</td>\n",
              "      <td>-0.627810</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.687726</td>\n",
              "      <td>0.510351</td>\n",
              "      <td>0.508974</td>\n",
              "      <td>0.634336</td>\n",
              "      <td>-0.263018</td>\n",
              "      <td>0.648615</td>\n",
              "      <td>0.707676</td>\n",
              "      <td>-0.688436</td>\n",
              "      <td>0.705497</td>\n",
              "      <td>-0.019272</td>\n",
              "      <td>0.673179</td>\n",
              "      <td>0.098516</td>\n",
              "      <td>0.614504</td>\n",
              "      <td>0.578223</td>\n",
              "      <td>0.525509</td>\n",
              "      <td>-0.314396</td>\n",
              "      <td>-0.253805</td>\n",
              "      <td>-0.021710</td>\n",
              "      <td>-0.041970</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.076460</td>\n",
              "      <td>-0.076332</td>\n",
              "      <td>-0.052056</td>\n",
              "      <td>-0.190582</td>\n",
              "      <td>0.005346</td>\n",
              "      <td>-0.002001</td>\n",
              "      <td>-0.682095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>0.617554</td>\n",
              "      <td>-0.498963</td>\n",
              "      <td>0.579638</td>\n",
              "      <td>-0.687726</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.429243</td>\n",
              "      <td>-0.474403</td>\n",
              "      <td>-0.588648</td>\n",
              "      <td>0.199013</td>\n",
              "      <td>-0.676648</td>\n",
              "      <td>-0.712839</td>\n",
              "      <td>0.708642</td>\n",
              "      <td>-0.722597</td>\n",
              "      <td>0.011519</td>\n",
              "      <td>-0.714847</td>\n",
              "      <td>-0.098627</td>\n",
              "      <td>-0.593948</td>\n",
              "      <td>-0.532786</td>\n",
              "      <td>-0.482267</td>\n",
              "      <td>0.269842</td>\n",
              "      <td>0.257236</td>\n",
              "      <td>-0.013093</td>\n",
              "      <td>0.091197</td>\n",
              "      <td>0.043266</td>\n",
              "      <td>-0.102508</td>\n",
              "      <td>0.029402</td>\n",
              "      <td>0.136679</td>\n",
              "      <td>0.188036</td>\n",
              "      <td>-0.011316</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>0.735981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>-0.268445</td>\n",
              "      <td>0.517462</td>\n",
              "      <td>-0.631669</td>\n",
              "      <td>0.510351</td>\n",
              "      <td>-0.429243</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.245187</td>\n",
              "      <td>0.586828</td>\n",
              "      <td>-0.314975</td>\n",
              "      <td>0.479614</td>\n",
              "      <td>0.563874</td>\n",
              "      <td>-0.440100</td>\n",
              "      <td>0.473002</td>\n",
              "      <td>-0.115317</td>\n",
              "      <td>0.387454</td>\n",
              "      <td>0.058686</td>\n",
              "      <td>0.596898</td>\n",
              "      <td>0.669625</td>\n",
              "      <td>0.645095</td>\n",
              "      <td>-0.438118</td>\n",
              "      <td>-0.246694</td>\n",
              "      <td>0.034147</td>\n",
              "      <td>-0.119152</td>\n",
              "      <td>-0.113919</td>\n",
              "      <td>-0.083243</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>0.047771</td>\n",
              "      <td>-0.043759</td>\n",
              "      <td>0.108422</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.338639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>-0.387916</td>\n",
              "      <td>0.354728</td>\n",
              "      <td>-0.341040</td>\n",
              "      <td>0.508974</td>\n",
              "      <td>-0.474403</td>\n",
              "      <td>0.245187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.418703</td>\n",
              "      <td>-0.604491</td>\n",
              "      <td>0.432241</td>\n",
              "      <td>0.471000</td>\n",
              "      <td>-0.497611</td>\n",
              "      <td>0.498993</td>\n",
              "      <td>-0.117637</td>\n",
              "      <td>0.510123</td>\n",
              "      <td>-0.023851</td>\n",
              "      <td>0.415834</td>\n",
              "      <td>0.378152</td>\n",
              "      <td>0.328019</td>\n",
              "      <td>-0.235623</td>\n",
              "      <td>-0.188360</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.036896</td>\n",
              "      <td>0.308598</td>\n",
              "      <td>-0.005237</td>\n",
              "      <td>-0.195340</td>\n",
              "      <td>-0.067605</td>\n",
              "      <td>-0.260783</td>\n",
              "      <td>-0.065641</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>-0.435088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>-0.414288</td>\n",
              "      <td>0.573381</td>\n",
              "      <td>-0.694022</td>\n",
              "      <td>0.634336</td>\n",
              "      <td>-0.588648</td>\n",
              "      <td>0.586828</td>\n",
              "      <td>0.418703</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>0.601789</td>\n",
              "      <td>0.678004</td>\n",
              "      <td>-0.587660</td>\n",
              "      <td>0.603318</td>\n",
              "      <td>-0.030000</td>\n",
              "      <td>0.535612</td>\n",
              "      <td>0.135939</td>\n",
              "      <td>0.667244</td>\n",
              "      <td>0.655755</td>\n",
              "      <td>0.625680</td>\n",
              "      <td>-0.372270</td>\n",
              "      <td>-0.299436</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>-0.104043</td>\n",
              "      <td>-0.111177</td>\n",
              "      <td>-0.004152</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>-0.006488</td>\n",
              "      <td>-0.036557</td>\n",
              "      <td>0.040732</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>-0.491234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>0.121282</td>\n",
              "      <td>-0.226757</td>\n",
              "      <td>0.191321</td>\n",
              "      <td>-0.263018</td>\n",
              "      <td>0.199013</td>\n",
              "      <td>-0.314975</td>\n",
              "      <td>-0.604491</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.208557</td>\n",
              "      <td>-0.199995</td>\n",
              "      <td>0.223052</td>\n",
              "      <td>-0.211999</td>\n",
              "      <td>0.273958</td>\n",
              "      <td>-0.216410</td>\n",
              "      <td>0.101690</td>\n",
              "      <td>-0.230638</td>\n",
              "      <td>-0.277246</td>\n",
              "      <td>-0.249986</td>\n",
              "      <td>0.253272</td>\n",
              "      <td>0.131354</td>\n",
              "      <td>0.056416</td>\n",
              "      <td>-0.098752</td>\n",
              "      <td>-0.463649</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.322639</td>\n",
              "      <td>0.040448</td>\n",
              "      <td>0.298398</td>\n",
              "      <td>0.046017</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.144294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>-0.508427</td>\n",
              "      <td>0.548973</td>\n",
              "      <td>-0.585095</td>\n",
              "      <td>0.648615</td>\n",
              "      <td>-0.676648</td>\n",
              "      <td>0.479614</td>\n",
              "      <td>0.432241</td>\n",
              "      <td>0.601789</td>\n",
              "      <td>-0.208557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.748487</td>\n",
              "      <td>-0.633556</td>\n",
              "      <td>0.667266</td>\n",
              "      <td>-0.006167</td>\n",
              "      <td>0.633212</td>\n",
              "      <td>0.114613</td>\n",
              "      <td>0.573957</td>\n",
              "      <td>0.581604</td>\n",
              "      <td>0.522720</td>\n",
              "      <td>-0.294432</td>\n",
              "      <td>-0.328975</td>\n",
              "      <td>0.131001</td>\n",
              "      <td>-0.204723</td>\n",
              "      <td>-0.042371</td>\n",
              "      <td>0.044006</td>\n",
              "      <td>-0.034885</td>\n",
              "      <td>-0.131000</td>\n",
              "      <td>-0.111842</td>\n",
              "      <td>0.069959</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.585522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V10</th>\n",
              "      <td>-0.578014</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>-0.621798</td>\n",
              "      <td>0.707676</td>\n",
              "      <td>-0.712839</td>\n",
              "      <td>0.563874</td>\n",
              "      <td>0.471000</td>\n",
              "      <td>0.678004</td>\n",
              "      <td>-0.199995</td>\n",
              "      <td>0.748487</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.713066</td>\n",
              "      <td>0.736783</td>\n",
              "      <td>-0.019246</td>\n",
              "      <td>0.698939</td>\n",
              "      <td>0.111051</td>\n",
              "      <td>0.686602</td>\n",
              "      <td>0.649149</td>\n",
              "      <td>0.596702</td>\n",
              "      <td>-0.375080</td>\n",
              "      <td>-0.287051</td>\n",
              "      <td>0.037426</td>\n",
              "      <td>-0.150957</td>\n",
              "      <td>-0.056285</td>\n",
              "      <td>0.045935</td>\n",
              "      <td>-0.014045</td>\n",
              "      <td>-0.053684</td>\n",
              "      <td>-0.134907</td>\n",
              "      <td>0.035646</td>\n",
              "      <td>-0.001259</td>\n",
              "      <td>-0.673665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V11</th>\n",
              "      <td>0.589321</td>\n",
              "      <td>-0.525797</td>\n",
              "      <td>0.558863</td>\n",
              "      <td>-0.688436</td>\n",
              "      <td>0.708642</td>\n",
              "      <td>-0.440100</td>\n",
              "      <td>-0.497611</td>\n",
              "      <td>-0.587660</td>\n",
              "      <td>0.223052</td>\n",
              "      <td>-0.633556</td>\n",
              "      <td>-0.713066</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.744642</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>-0.762322</td>\n",
              "      <td>-0.058089</td>\n",
              "      <td>-0.655081</td>\n",
              "      <td>-0.601924</td>\n",
              "      <td>-0.519721</td>\n",
              "      <td>0.346170</td>\n",
              "      <td>0.204378</td>\n",
              "      <td>0.111608</td>\n",
              "      <td>0.022153</td>\n",
              "      <td>0.013596</td>\n",
              "      <td>-0.104340</td>\n",
              "      <td>0.051535</td>\n",
              "      <td>0.133635</td>\n",
              "      <td>0.290912</td>\n",
              "      <td>0.059732</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.724278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V12</th>\n",
              "      <td>-0.652940</td>\n",
              "      <td>0.580715</td>\n",
              "      <td>-0.574935</td>\n",
              "      <td>0.705497</td>\n",
              "      <td>-0.722597</td>\n",
              "      <td>0.473002</td>\n",
              "      <td>0.498993</td>\n",
              "      <td>0.603318</td>\n",
              "      <td>-0.211999</td>\n",
              "      <td>0.667266</td>\n",
              "      <td>0.736783</td>\n",
              "      <td>-0.744642</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.018336</td>\n",
              "      <td>0.783878</td>\n",
              "      <td>0.041208</td>\n",
              "      <td>0.698490</td>\n",
              "      <td>0.658739</td>\n",
              "      <td>0.579374</td>\n",
              "      <td>-0.368161</td>\n",
              "      <td>-0.219275</td>\n",
              "      <td>-0.080394</td>\n",
              "      <td>-0.072096</td>\n",
              "      <td>-0.019261</td>\n",
              "      <td>0.080407</td>\n",
              "      <td>-0.010350</td>\n",
              "      <td>-0.114272</td>\n",
              "      <td>-0.216563</td>\n",
              "      <td>-0.053136</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.768579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>-0.076331</td>\n",
              "      <td>-0.020567</td>\n",
              "      <td>0.012801</td>\n",
              "      <td>-0.019272</td>\n",
              "      <td>0.011519</td>\n",
              "      <td>-0.115317</td>\n",
              "      <td>-0.117637</td>\n",
              "      <td>-0.030000</td>\n",
              "      <td>0.273958</td>\n",
              "      <td>-0.006167</td>\n",
              "      <td>-0.019246</td>\n",
              "      <td>0.014518</td>\n",
              "      <td>0.018336</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.028738</td>\n",
              "      <td>-0.016044</td>\n",
              "      <td>-0.082089</td>\n",
              "      <td>-0.123173</td>\n",
              "      <td>-0.122049</td>\n",
              "      <td>0.167676</td>\n",
              "      <td>-0.007267</td>\n",
              "      <td>0.025529</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>0.060097</td>\n",
              "      <td>0.003580</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.058483</td>\n",
              "      <td>-0.101488</td>\n",
              "      <td>-0.002718</td>\n",
              "      <td>-0.071105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V14</th>\n",
              "      <td>-0.709346</td>\n",
              "      <td>0.494427</td>\n",
              "      <td>-0.523294</td>\n",
              "      <td>0.673179</td>\n",
              "      <td>-0.714847</td>\n",
              "      <td>0.387454</td>\n",
              "      <td>0.510123</td>\n",
              "      <td>0.535612</td>\n",
              "      <td>-0.216410</td>\n",
              "      <td>0.633212</td>\n",
              "      <td>0.698939</td>\n",
              "      <td>-0.762322</td>\n",
              "      <td>0.783878</td>\n",
              "      <td>0.028738</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010011</td>\n",
              "      <td>0.630516</td>\n",
              "      <td>0.552428</td>\n",
              "      <td>0.469393</td>\n",
              "      <td>-0.315192</td>\n",
              "      <td>-0.148104</td>\n",
              "      <td>-0.189902</td>\n",
              "      <td>0.052023</td>\n",
              "      <td>-0.007601</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>-0.087040</td>\n",
              "      <td>-0.142472</td>\n",
              "      <td>-0.299951</td>\n",
              "      <td>-0.127969</td>\n",
              "      <td>-0.001363</td>\n",
              "      <td>-0.805669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>-0.080004</td>\n",
              "      <td>0.046002</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>0.098516</td>\n",
              "      <td>-0.098627</td>\n",
              "      <td>0.058686</td>\n",
              "      <td>-0.023851</td>\n",
              "      <td>0.135939</td>\n",
              "      <td>0.101690</td>\n",
              "      <td>0.114613</td>\n",
              "      <td>0.111051</td>\n",
              "      <td>-0.058089</td>\n",
              "      <td>0.041208</td>\n",
              "      <td>-0.016044</td>\n",
              "      <td>0.010011</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.031912</td>\n",
              "      <td>0.017136</td>\n",
              "      <td>0.185638</td>\n",
              "      <td>-0.135423</td>\n",
              "      <td>0.171719</td>\n",
              "      <td>-0.099347</td>\n",
              "      <td>-0.074832</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>-0.027579</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>0.116106</td>\n",
              "      <td>0.100293</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>-0.037948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>-0.494255</td>\n",
              "      <td>0.621884</td>\n",
              "      <td>-0.534392</td>\n",
              "      <td>0.614504</td>\n",
              "      <td>-0.593948</td>\n",
              "      <td>0.596898</td>\n",
              "      <td>0.415834</td>\n",
              "      <td>0.667244</td>\n",
              "      <td>-0.230638</td>\n",
              "      <td>0.573957</td>\n",
              "      <td>0.686602</td>\n",
              "      <td>-0.655081</td>\n",
              "      <td>0.698490</td>\n",
              "      <td>-0.082089</td>\n",
              "      <td>0.630516</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.848095</td>\n",
              "      <td>0.767992</td>\n",
              "      <td>-0.585409</td>\n",
              "      <td>-0.223431</td>\n",
              "      <td>-0.117591</td>\n",
              "      <td>-0.101847</td>\n",
              "      <td>-0.057100</td>\n",
              "      <td>-0.023511</td>\n",
              "      <td>0.062484</td>\n",
              "      <td>-0.056184</td>\n",
              "      <td>-0.191742</td>\n",
              "      <td>-0.022328</td>\n",
              "      <td>-0.000479</td>\n",
              "      <td>-0.573511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V17</th>\n",
              "      <td>-0.417226</td>\n",
              "      <td>0.605799</td>\n",
              "      <td>-0.495836</td>\n",
              "      <td>0.578223</td>\n",
              "      <td>-0.532786</td>\n",
              "      <td>0.669625</td>\n",
              "      <td>0.378152</td>\n",
              "      <td>0.655755</td>\n",
              "      <td>-0.277246</td>\n",
              "      <td>0.581604</td>\n",
              "      <td>0.649149</td>\n",
              "      <td>-0.601924</td>\n",
              "      <td>0.658739</td>\n",
              "      <td>-0.123173</td>\n",
              "      <td>0.552428</td>\n",
              "      <td>0.031912</td>\n",
              "      <td>0.848095</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.851366</td>\n",
              "      <td>-0.614222</td>\n",
              "      <td>-0.215708</td>\n",
              "      <td>-0.079348</td>\n",
              "      <td>-0.144637</td>\n",
              "      <td>-0.044635</td>\n",
              "      <td>-0.072198</td>\n",
              "      <td>0.075609</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>-0.184550</td>\n",
              "      <td>0.019570</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.476377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V18</th>\n",
              "      <td>-0.341056</td>\n",
              "      <td>0.577296</td>\n",
              "      <td>-0.482162</td>\n",
              "      <td>0.525509</td>\n",
              "      <td>-0.482267</td>\n",
              "      <td>0.645095</td>\n",
              "      <td>0.328019</td>\n",
              "      <td>0.625680</td>\n",
              "      <td>-0.249986</td>\n",
              "      <td>0.522720</td>\n",
              "      <td>0.596702</td>\n",
              "      <td>-0.519721</td>\n",
              "      <td>0.579374</td>\n",
              "      <td>-0.122049</td>\n",
              "      <td>0.469393</td>\n",
              "      <td>0.017136</td>\n",
              "      <td>0.767992</td>\n",
              "      <td>0.851366</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.565991</td>\n",
              "      <td>-0.185670</td>\n",
              "      <td>-0.060862</td>\n",
              "      <td>-0.135994</td>\n",
              "      <td>-0.046262</td>\n",
              "      <td>-0.099745</td>\n",
              "      <td>0.070467</td>\n",
              "      <td>-0.021039</td>\n",
              "      <td>-0.141790</td>\n",
              "      <td>0.052547</td>\n",
              "      <td>-0.001516</td>\n",
              "      <td>-0.410091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>0.216276</td>\n",
              "      <td>-0.377803</td>\n",
              "      <td>0.208821</td>\n",
              "      <td>-0.314396</td>\n",
              "      <td>0.269842</td>\n",
              "      <td>-0.438118</td>\n",
              "      <td>-0.235623</td>\n",
              "      <td>-0.372270</td>\n",
              "      <td>0.253272</td>\n",
              "      <td>-0.294432</td>\n",
              "      <td>-0.375080</td>\n",
              "      <td>0.346170</td>\n",
              "      <td>-0.368161</td>\n",
              "      <td>0.167676</td>\n",
              "      <td>-0.315192</td>\n",
              "      <td>0.185638</td>\n",
              "      <td>-0.585409</td>\n",
              "      <td>-0.614222</td>\n",
              "      <td>-0.565991</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.071641</td>\n",
              "      <td>0.136080</td>\n",
              "      <td>0.110066</td>\n",
              "      <td>-0.001529</td>\n",
              "      <td>0.110751</td>\n",
              "      <td>-0.174328</td>\n",
              "      <td>0.041421</td>\n",
              "      <td>0.123266</td>\n",
              "      <td>-0.024368</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>0.244081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>0.145803</td>\n",
              "      <td>-0.219164</td>\n",
              "      <td>0.263707</td>\n",
              "      <td>-0.253805</td>\n",
              "      <td>0.257236</td>\n",
              "      <td>-0.246694</td>\n",
              "      <td>-0.188360</td>\n",
              "      <td>-0.299436</td>\n",
              "      <td>0.131354</td>\n",
              "      <td>-0.328975</td>\n",
              "      <td>-0.287051</td>\n",
              "      <td>0.204378</td>\n",
              "      <td>-0.219275</td>\n",
              "      <td>-0.007267</td>\n",
              "      <td>-0.148104</td>\n",
              "      <td>-0.135423</td>\n",
              "      <td>-0.223431</td>\n",
              "      <td>-0.215708</td>\n",
              "      <td>-0.185670</td>\n",
              "      <td>0.071641</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.529918</td>\n",
              "      <td>0.429362</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>-0.020316</td>\n",
              "      <td>0.030478</td>\n",
              "      <td>0.007677</td>\n",
              "      <td>-0.055183</td>\n",
              "      <td>-0.035727</td>\n",
              "      <td>-0.001405</td>\n",
              "      <td>0.179851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>0.097948</td>\n",
              "      <td>-0.034669</td>\n",
              "      <td>-0.013570</td>\n",
              "      <td>-0.021710</td>\n",
              "      <td>-0.013093</td>\n",
              "      <td>0.034147</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>0.056416</td>\n",
              "      <td>0.131001</td>\n",
              "      <td>0.037426</td>\n",
              "      <td>0.111608</td>\n",
              "      <td>-0.080394</td>\n",
              "      <td>0.025529</td>\n",
              "      <td>-0.189902</td>\n",
              "      <td>0.171719</td>\n",
              "      <td>-0.117591</td>\n",
              "      <td>-0.079348</td>\n",
              "      <td>-0.060862</td>\n",
              "      <td>0.136080</td>\n",
              "      <td>-0.529918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.734653</td>\n",
              "      <td>0.096587</td>\n",
              "      <td>-0.059190</td>\n",
              "      <td>0.146164</td>\n",
              "      <td>0.070050</td>\n",
              "      <td>0.373256</td>\n",
              "      <td>0.326677</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.109640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V22</th>\n",
              "      <td>0.036106</td>\n",
              "      <td>-0.073729</td>\n",
              "      <td>0.035346</td>\n",
              "      <td>-0.041970</td>\n",
              "      <td>0.091197</td>\n",
              "      <td>-0.119152</td>\n",
              "      <td>0.036896</td>\n",
              "      <td>-0.104043</td>\n",
              "      <td>-0.098752</td>\n",
              "      <td>-0.204723</td>\n",
              "      <td>-0.150957</td>\n",
              "      <td>0.022153</td>\n",
              "      <td>-0.072096</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.052023</td>\n",
              "      <td>-0.099347</td>\n",
              "      <td>-0.101847</td>\n",
              "      <td>-0.144637</td>\n",
              "      <td>-0.135994</td>\n",
              "      <td>0.110066</td>\n",
              "      <td>0.429362</td>\n",
              "      <td>-0.734653</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.000636</td>\n",
              "      <td>0.079790</td>\n",
              "      <td>-0.258956</td>\n",
              "      <td>-0.015127</td>\n",
              "      <td>-0.340640</td>\n",
              "      <td>-0.282893</td>\n",
              "      <td>-0.000942</td>\n",
              "      <td>0.014098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>0.017594</td>\n",
              "      <td>-0.068917</td>\n",
              "      <td>0.151906</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.043266</td>\n",
              "      <td>-0.113919</td>\n",
              "      <td>0.308598</td>\n",
              "      <td>-0.111177</td>\n",
              "      <td>-0.463649</td>\n",
              "      <td>-0.042371</td>\n",
              "      <td>-0.056285</td>\n",
              "      <td>0.013596</td>\n",
              "      <td>-0.019261</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.007601</td>\n",
              "      <td>-0.074832</td>\n",
              "      <td>-0.057100</td>\n",
              "      <td>-0.044635</td>\n",
              "      <td>-0.046262</td>\n",
              "      <td>-0.001529</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>0.096587</td>\n",
              "      <td>-0.000636</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.051181</td>\n",
              "      <td>-0.040882</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>-0.151698</td>\n",
              "      <td>0.028059</td>\n",
              "      <td>-0.001981</td>\n",
              "      <td>0.010255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>-0.116685</td>\n",
              "      <td>-0.014651</td>\n",
              "      <td>-0.027515</td>\n",
              "      <td>0.076460</td>\n",
              "      <td>-0.102508</td>\n",
              "      <td>-0.083243</td>\n",
              "      <td>-0.005237</td>\n",
              "      <td>-0.004152</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.044006</td>\n",
              "      <td>0.045935</td>\n",
              "      <td>-0.104340</td>\n",
              "      <td>0.080407</td>\n",
              "      <td>0.060097</td>\n",
              "      <td>0.138718</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>-0.023511</td>\n",
              "      <td>-0.072198</td>\n",
              "      <td>-0.099745</td>\n",
              "      <td>0.110751</td>\n",
              "      <td>-0.020316</td>\n",
              "      <td>-0.059190</td>\n",
              "      <td>0.079790</td>\n",
              "      <td>-0.051181</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.079604</td>\n",
              "      <td>-0.113362</td>\n",
              "      <td>-0.194899</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>-0.000846</td>\n",
              "      <td>-0.130107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>0.005586</td>\n",
              "      <td>-0.008508</td>\n",
              "      <td>0.132443</td>\n",
              "      <td>-0.076332</td>\n",
              "      <td>0.029402</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>-0.195340</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.322639</td>\n",
              "      <td>-0.034885</td>\n",
              "      <td>-0.014045</td>\n",
              "      <td>0.051535</td>\n",
              "      <td>-0.010350</td>\n",
              "      <td>0.003580</td>\n",
              "      <td>-0.087040</td>\n",
              "      <td>-0.027579</td>\n",
              "      <td>0.062484</td>\n",
              "      <td>0.075609</td>\n",
              "      <td>0.070467</td>\n",
              "      <td>-0.174328</td>\n",
              "      <td>0.030478</td>\n",
              "      <td>0.146164</td>\n",
              "      <td>-0.258956</td>\n",
              "      <td>-0.040882</td>\n",
              "      <td>-0.079604</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.057546</td>\n",
              "      <td>0.215653</td>\n",
              "      <td>0.176058</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>0.061847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>0.052126</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>-0.052056</td>\n",
              "      <td>0.136679</td>\n",
              "      <td>0.047771</td>\n",
              "      <td>-0.067605</td>\n",
              "      <td>-0.006488</td>\n",
              "      <td>0.040448</td>\n",
              "      <td>-0.131000</td>\n",
              "      <td>-0.053684</td>\n",
              "      <td>0.133635</td>\n",
              "      <td>-0.114272</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>-0.142472</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>-0.056184</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>-0.021039</td>\n",
              "      <td>0.041421</td>\n",
              "      <td>0.007677</td>\n",
              "      <td>0.070050</td>\n",
              "      <td>-0.015127</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>-0.113362</td>\n",
              "      <td>0.057546</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.193977</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>0.071052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>0.184195</td>\n",
              "      <td>-0.122772</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>-0.190582</td>\n",
              "      <td>0.188036</td>\n",
              "      <td>-0.043759</td>\n",
              "      <td>-0.260783</td>\n",
              "      <td>-0.036557</td>\n",
              "      <td>0.298398</td>\n",
              "      <td>-0.111842</td>\n",
              "      <td>-0.134907</td>\n",
              "      <td>0.290912</td>\n",
              "      <td>-0.216563</td>\n",
              "      <td>0.058483</td>\n",
              "      <td>-0.299951</td>\n",
              "      <td>0.116106</td>\n",
              "      <td>-0.191742</td>\n",
              "      <td>-0.184550</td>\n",
              "      <td>-0.141790</td>\n",
              "      <td>0.123266</td>\n",
              "      <td>-0.055183</td>\n",
              "      <td>0.373256</td>\n",
              "      <td>-0.340640</td>\n",
              "      <td>-0.151698</td>\n",
              "      <td>-0.194899</td>\n",
              "      <td>0.215653</td>\n",
              "      <td>0.193977</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.214002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.070111</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>0.005346</td>\n",
              "      <td>-0.011316</td>\n",
              "      <td>0.108422</td>\n",
              "      <td>-0.065641</td>\n",
              "      <td>0.040732</td>\n",
              "      <td>0.046017</td>\n",
              "      <td>0.069959</td>\n",
              "      <td>0.035646</td>\n",
              "      <td>0.059732</td>\n",
              "      <td>-0.053136</td>\n",
              "      <td>-0.101488</td>\n",
              "      <td>-0.127969</td>\n",
              "      <td>0.100293</td>\n",
              "      <td>-0.022328</td>\n",
              "      <td>0.019570</td>\n",
              "      <td>0.052547</td>\n",
              "      <td>-0.024368</td>\n",
              "      <td>-0.035727</td>\n",
              "      <td>0.326677</td>\n",
              "      <td>-0.282893</td>\n",
              "      <td>0.028059</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>0.176058</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.001503</td>\n",
              "      <td>0.102024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>0.001710</td>\n",
              "      <td>-0.001280</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>-0.002001</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.001259</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>-0.001245</td>\n",
              "      <td>-0.002718</td>\n",
              "      <td>-0.001363</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>-0.000479</td>\n",
              "      <td>-0.000358</td>\n",
              "      <td>-0.001516</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>-0.001405</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>-0.000942</td>\n",
              "      <td>-0.001981</td>\n",
              "      <td>-0.000846</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>-0.001503</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <td>0.864283</td>\n",
              "      <td>-0.505761</td>\n",
              "      <td>0.491878</td>\n",
              "      <td>-0.682095</td>\n",
              "      <td>0.735981</td>\n",
              "      <td>-0.338639</td>\n",
              "      <td>-0.435088</td>\n",
              "      <td>-0.491234</td>\n",
              "      <td>0.144294</td>\n",
              "      <td>-0.585522</td>\n",
              "      <td>-0.673665</td>\n",
              "      <td>0.724278</td>\n",
              "      <td>-0.768579</td>\n",
              "      <td>-0.071105</td>\n",
              "      <td>-0.805669</td>\n",
              "      <td>-0.037948</td>\n",
              "      <td>-0.573511</td>\n",
              "      <td>-0.476377</td>\n",
              "      <td>-0.410091</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.179851</td>\n",
              "      <td>0.109640</td>\n",
              "      <td>0.014098</td>\n",
              "      <td>0.010255</td>\n",
              "      <td>-0.130107</td>\n",
              "      <td>0.061847</td>\n",
              "      <td>0.071052</td>\n",
              "      <td>0.214002</td>\n",
              "      <td>0.102024</td>\n",
              "      <td>0.002261</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3b48174-9e0d-4cbd-8d75-e838bd5842a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3b48174-9e0d-4cbd-8d75-e838bd5842a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3b48174-9e0d-4cbd-8d75-e838bd5842a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b90d2cd0-06ae-4846-b874-99194bf325fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b90d2cd0-06ae-4846-b874-99194bf325fd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b90d2cd0-06ae-4846-b874-99194bf325fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.corr())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "qwBgUAnUIt5H",
        "outputId": "8dcfa472-e5ac-489e-e0c0-dbc319d61641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new=df.copy()"
      ],
      "metadata": {
        "id": "FiKSQDCbT8c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying a filter for columns with inter co-relation**"
      ],
      "metadata": {
        "id": "30tvpZgVlmyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop('Class', axis=1)\n",
        "\n",
        "correlation_matrix = features.corr()\n",
        "\n",
        "correlation_threshold = 0.7\n",
        "\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "print(\"Highly Correlated Features:\", highly_correlated_features)\n",
        "\n",
        "features_filtered = features.drop(highly_correlated_features, axis=1)\n",
        "filtered_df = pd.concat([features_filtered, df['Class']], axis=1)\n",
        "\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(filtered_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkh44KwTTiNQ",
        "outputId": "9ace81ef-a639-4fd5-cd89-0865493df2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly Correlated Features: {'V18', 'V12', 'V17', 'V10', 'V11', 'V22', 'V14'}\n",
            "Filtered DataFrame:\n",
            "   id        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
            "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
            "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
            "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
            "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
            "\n",
            "         V8        V9       V13       V15       V16       V19       V20  \\\n",
            "0 -0.130006  0.727159 -0.941386  1.804879  0.215598  0.124270  0.091202   \n",
            "1 -0.133118  0.347452  0.574074  0.706121  0.789188 -0.340687 -0.233984   \n",
            "2 -0.095576 -0.261297  0.805173  3.069025 -0.577514 -2.366079  0.361652   \n",
            "3 -0.065130 -0.205698  0.592994 -0.697664 -0.030669 -1.345060 -0.378223   \n",
            "4 -0.212660  1.049921  1.439310  0.153008  0.224538  0.445317  0.247237   \n",
            "\n",
            "        V21       V23       V24       V25       V26       V27       V28  \\\n",
            "0 -0.110552 -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045   \n",
            "1 -0.194936  0.079469 -0.577395  0.190090  0.296503 -0.248052 -0.064512   \n",
            "2 -0.005020  0.945045 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718   \n",
            "3 -0.146927 -0.214048 -1.893131  1.003963 -0.515950 -0.165316  0.048424   \n",
            "4 -0.106984 -0.161666  0.312561 -0.414116  1.071126  0.023712  0.419117   \n",
            "\n",
            "     Amount  Class  \n",
            "0  17982.10      0  \n",
            "1   6531.37      0  \n",
            "2   2513.54      0  \n",
            "3   5384.44      0  \n",
            "4  14278.97      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "vl3YG2WCZFMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.corr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "QDFbYkDEZ475",
        "outputId": "ee0488ad-97b9-4426-82b6-3f233229f096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              V1        V2        V3        V4        V5        V6        V7  \\\n",
              "V1      1.000000 -0.561184  0.484499 -0.498963  0.517462  0.354728  0.573381   \n",
              "V2     -0.561184  1.000000 -0.627810  0.579638 -0.631669 -0.341040 -0.694022   \n",
              "V3      0.484499 -0.627810  1.000000 -0.687726  0.510351  0.508974  0.634336   \n",
              "V4     -0.498963  0.579638 -0.687726  1.000000 -0.429243 -0.474403 -0.588648   \n",
              "V5      0.517462 -0.631669  0.510351 -0.429243  1.000000  0.245187  0.586828   \n",
              "V6      0.354728 -0.341040  0.508974 -0.474403  0.245187  1.000000  0.418703   \n",
              "V7      0.573381 -0.694022  0.634336 -0.588648  0.586828  0.418703  1.000000   \n",
              "V8     -0.226757  0.191321 -0.263018  0.199013 -0.314975 -0.604491 -0.180986   \n",
              "V9      0.548973 -0.585095  0.648615 -0.676648  0.479614  0.432241  0.601789   \n",
              "V13    -0.020567  0.012801 -0.019272  0.011519 -0.115317 -0.117637 -0.030000   \n",
              "V15     0.046002 -0.161325  0.098516 -0.098627  0.058686 -0.023851  0.135939   \n",
              "V16     0.621884 -0.534392  0.614504 -0.593948  0.596898  0.415834  0.667244   \n",
              "V19    -0.377803  0.208821 -0.314396  0.269842 -0.438118 -0.235623 -0.372270   \n",
              "V20    -0.219164  0.263707 -0.253805  0.257236 -0.246694 -0.188360 -0.299436   \n",
              "V21    -0.034669 -0.013570 -0.021710 -0.013093  0.034147 -0.040153  0.019627   \n",
              "V23    -0.068917  0.151906 -0.058884  0.043266 -0.113919  0.308598 -0.111177   \n",
              "V24    -0.014651 -0.027515  0.076460 -0.102508 -0.083243 -0.005237 -0.004152   \n",
              "V25    -0.008508  0.132443 -0.076332  0.029402 -0.047845 -0.195340  0.000802   \n",
              "V26     0.009281  0.012219 -0.052056  0.136679  0.047771 -0.067605 -0.006488   \n",
              "V27    -0.122772  0.053835 -0.190582  0.188036 -0.043759 -0.260783 -0.036557   \n",
              "V28     0.070111  0.021071  0.005346 -0.011316  0.108422 -0.065641  0.040732   \n",
              "Amount -0.001280 -0.000076 -0.002001  0.001859 -0.000016  0.000734  0.001326   \n",
              "Class  -0.505761  0.491878 -0.682095  0.735981 -0.338639 -0.435088 -0.491234   \n",
              "\n",
              "              V8        V9       V13       V15       V16       V19       V20  \\\n",
              "V1     -0.226757  0.548973 -0.020567  0.046002  0.621884 -0.377803 -0.219164   \n",
              "V2      0.191321 -0.585095  0.012801 -0.161325 -0.534392  0.208821  0.263707   \n",
              "V3     -0.263018  0.648615 -0.019272  0.098516  0.614504 -0.314396 -0.253805   \n",
              "V4      0.199013 -0.676648  0.011519 -0.098627 -0.593948  0.269842  0.257236   \n",
              "V5     -0.314975  0.479614 -0.115317  0.058686  0.596898 -0.438118 -0.246694   \n",
              "V6     -0.604491  0.432241 -0.117637 -0.023851  0.415834 -0.235623 -0.188360   \n",
              "V7     -0.180986  0.601789 -0.030000  0.135939  0.667244 -0.372270 -0.299436   \n",
              "V8      1.000000 -0.208557  0.273958  0.101690 -0.230638  0.253272  0.131354   \n",
              "V9     -0.208557  1.000000 -0.006167  0.114613  0.573957 -0.294432 -0.328975   \n",
              "V13     0.273958 -0.006167  1.000000 -0.016044 -0.082089  0.167676 -0.007267   \n",
              "V15     0.101690  0.114613 -0.016044  1.000000  0.001357  0.185638 -0.135423   \n",
              "V16    -0.230638  0.573957 -0.082089  0.001357  1.000000 -0.585409 -0.223431   \n",
              "V19     0.253272 -0.294432  0.167676  0.185638 -0.585409  1.000000  0.071641   \n",
              "V20     0.131354 -0.328975 -0.007267 -0.135423 -0.223431  0.071641  1.000000   \n",
              "V21     0.056416  0.131001  0.025529  0.171719 -0.117591  0.136080 -0.529918   \n",
              "V23    -0.463649 -0.042371 -0.123520 -0.074832 -0.057100 -0.001529  0.017204   \n",
              "V24     0.083272  0.044006  0.060097  0.023003 -0.023511  0.110751 -0.020316   \n",
              "V25     0.322639 -0.034885  0.003580 -0.027579  0.062484 -0.174328  0.030478   \n",
              "V26     0.040448 -0.131000  0.043750  0.047833 -0.056184  0.041421  0.007677   \n",
              "V27     0.298398 -0.111842  0.058483  0.116106 -0.191742  0.123266 -0.055183   \n",
              "V28     0.046017  0.069959 -0.101488  0.100293 -0.022328 -0.024368 -0.035727   \n",
              "Amount -0.000208 -0.001589 -0.002718  0.001190 -0.000479 -0.000400 -0.001405   \n",
              "Class   0.144294 -0.585522 -0.071105 -0.037948 -0.573511  0.244081  0.179851   \n",
              "\n",
              "             V21       V23       V24       V25       V26       V27       V28  \\\n",
              "V1     -0.034669 -0.068917 -0.014651 -0.008508  0.009281 -0.122772  0.070111   \n",
              "V2     -0.013570  0.151906 -0.027515  0.132443  0.012219  0.053835  0.021071   \n",
              "V3     -0.021710 -0.058884  0.076460 -0.076332 -0.052056 -0.190582  0.005346   \n",
              "V4     -0.013093  0.043266 -0.102508  0.029402  0.136679  0.188036 -0.011316   \n",
              "V5      0.034147 -0.113919 -0.083243 -0.047845  0.047771 -0.043759  0.108422   \n",
              "V6     -0.040153  0.308598 -0.005237 -0.195340 -0.067605 -0.260783 -0.065641   \n",
              "V7      0.019627 -0.111177 -0.004152  0.000802 -0.006488 -0.036557  0.040732   \n",
              "V8      0.056416 -0.463649  0.083272  0.322639  0.040448  0.298398  0.046017   \n",
              "V9      0.131001 -0.042371  0.044006 -0.034885 -0.131000 -0.111842  0.069959   \n",
              "V13     0.025529 -0.123520  0.060097  0.003580  0.043750  0.058483 -0.101488   \n",
              "V15     0.171719 -0.074832  0.023003 -0.027579  0.047833  0.116106  0.100293   \n",
              "V16    -0.117591 -0.057100 -0.023511  0.062484 -0.056184 -0.191742 -0.022328   \n",
              "V19     0.136080 -0.001529  0.110751 -0.174328  0.041421  0.123266 -0.024368   \n",
              "V20    -0.529918  0.017204 -0.020316  0.030478  0.007677 -0.055183 -0.035727   \n",
              "V21     1.000000  0.096587 -0.059190  0.146164  0.070050  0.373256  0.326677   \n",
              "V23     0.096587  1.000000 -0.051181 -0.040882  0.001057 -0.151698  0.028059   \n",
              "V24    -0.059190 -0.051181  1.000000 -0.079604 -0.113362 -0.194899 -0.045189   \n",
              "V25     0.146164 -0.040882 -0.079604  1.000000  0.057546  0.215653  0.176058   \n",
              "V26     0.070050  0.001057 -0.113362  0.057546  1.000000  0.193977  0.036830   \n",
              "V27     0.373256 -0.151698 -0.194899  0.215653  0.193977  1.000000  0.183233   \n",
              "V28     0.326677  0.028059 -0.045189  0.176058  0.036830  0.183233  1.000000   \n",
              "Amount  0.001029 -0.001981 -0.000846 -0.000720 -0.000120  0.001235 -0.001503   \n",
              "Class   0.109640  0.010255 -0.130107  0.061847  0.071052  0.214002  0.102024   \n",
              "\n",
              "          Amount     Class  \n",
              "V1     -0.001280 -0.505761  \n",
              "V2     -0.000076  0.491878  \n",
              "V3     -0.002001 -0.682095  \n",
              "V4      0.001859  0.735981  \n",
              "V5     -0.000016 -0.338639  \n",
              "V6      0.000734 -0.435088  \n",
              "V7      0.001326 -0.491234  \n",
              "V8     -0.000208  0.144294  \n",
              "V9     -0.001589 -0.585522  \n",
              "V13    -0.002718 -0.071105  \n",
              "V15     0.001190 -0.037948  \n",
              "V16    -0.000479 -0.573511  \n",
              "V19    -0.000400  0.244081  \n",
              "V20    -0.001405  0.179851  \n",
              "V21     0.001029  0.109640  \n",
              "V23    -0.001981  0.010255  \n",
              "V24    -0.000846 -0.130107  \n",
              "V25    -0.000720  0.061847  \n",
              "V26    -0.000120  0.071052  \n",
              "V27     0.001235  0.214002  \n",
              "V28    -0.001503  0.102024  \n",
              "Amount  1.000000  0.002261  \n",
              "Class   0.002261  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0079fea1-f224-483c-92ec-0f00ff7b2311\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V13</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.561184</td>\n",
              "      <td>0.484499</td>\n",
              "      <td>-0.498963</td>\n",
              "      <td>0.517462</td>\n",
              "      <td>0.354728</td>\n",
              "      <td>0.573381</td>\n",
              "      <td>-0.226757</td>\n",
              "      <td>0.548973</td>\n",
              "      <td>-0.020567</td>\n",
              "      <td>0.046002</td>\n",
              "      <td>0.621884</td>\n",
              "      <td>-0.377803</td>\n",
              "      <td>-0.219164</td>\n",
              "      <td>-0.034669</td>\n",
              "      <td>-0.068917</td>\n",
              "      <td>-0.014651</td>\n",
              "      <td>-0.008508</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>-0.122772</td>\n",
              "      <td>0.070111</td>\n",
              "      <td>-0.001280</td>\n",
              "      <td>-0.505761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>-0.561184</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.627810</td>\n",
              "      <td>0.579638</td>\n",
              "      <td>-0.631669</td>\n",
              "      <td>-0.341040</td>\n",
              "      <td>-0.694022</td>\n",
              "      <td>0.191321</td>\n",
              "      <td>-0.585095</td>\n",
              "      <td>0.012801</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>-0.534392</td>\n",
              "      <td>0.208821</td>\n",
              "      <td>0.263707</td>\n",
              "      <td>-0.013570</td>\n",
              "      <td>0.151906</td>\n",
              "      <td>-0.027515</td>\n",
              "      <td>0.132443</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>0.491878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>0.484499</td>\n",
              "      <td>-0.627810</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.687726</td>\n",
              "      <td>0.510351</td>\n",
              "      <td>0.508974</td>\n",
              "      <td>0.634336</td>\n",
              "      <td>-0.263018</td>\n",
              "      <td>0.648615</td>\n",
              "      <td>-0.019272</td>\n",
              "      <td>0.098516</td>\n",
              "      <td>0.614504</td>\n",
              "      <td>-0.314396</td>\n",
              "      <td>-0.253805</td>\n",
              "      <td>-0.021710</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.076460</td>\n",
              "      <td>-0.076332</td>\n",
              "      <td>-0.052056</td>\n",
              "      <td>-0.190582</td>\n",
              "      <td>0.005346</td>\n",
              "      <td>-0.002001</td>\n",
              "      <td>-0.682095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>-0.498963</td>\n",
              "      <td>0.579638</td>\n",
              "      <td>-0.687726</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.429243</td>\n",
              "      <td>-0.474403</td>\n",
              "      <td>-0.588648</td>\n",
              "      <td>0.199013</td>\n",
              "      <td>-0.676648</td>\n",
              "      <td>0.011519</td>\n",
              "      <td>-0.098627</td>\n",
              "      <td>-0.593948</td>\n",
              "      <td>0.269842</td>\n",
              "      <td>0.257236</td>\n",
              "      <td>-0.013093</td>\n",
              "      <td>0.043266</td>\n",
              "      <td>-0.102508</td>\n",
              "      <td>0.029402</td>\n",
              "      <td>0.136679</td>\n",
              "      <td>0.188036</td>\n",
              "      <td>-0.011316</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>0.735981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>0.517462</td>\n",
              "      <td>-0.631669</td>\n",
              "      <td>0.510351</td>\n",
              "      <td>-0.429243</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.245187</td>\n",
              "      <td>0.586828</td>\n",
              "      <td>-0.314975</td>\n",
              "      <td>0.479614</td>\n",
              "      <td>-0.115317</td>\n",
              "      <td>0.058686</td>\n",
              "      <td>0.596898</td>\n",
              "      <td>-0.438118</td>\n",
              "      <td>-0.246694</td>\n",
              "      <td>0.034147</td>\n",
              "      <td>-0.113919</td>\n",
              "      <td>-0.083243</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>0.047771</td>\n",
              "      <td>-0.043759</td>\n",
              "      <td>0.108422</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.338639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>0.354728</td>\n",
              "      <td>-0.341040</td>\n",
              "      <td>0.508974</td>\n",
              "      <td>-0.474403</td>\n",
              "      <td>0.245187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.418703</td>\n",
              "      <td>-0.604491</td>\n",
              "      <td>0.432241</td>\n",
              "      <td>-0.117637</td>\n",
              "      <td>-0.023851</td>\n",
              "      <td>0.415834</td>\n",
              "      <td>-0.235623</td>\n",
              "      <td>-0.188360</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.308598</td>\n",
              "      <td>-0.005237</td>\n",
              "      <td>-0.195340</td>\n",
              "      <td>-0.067605</td>\n",
              "      <td>-0.260783</td>\n",
              "      <td>-0.065641</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>-0.435088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>0.573381</td>\n",
              "      <td>-0.694022</td>\n",
              "      <td>0.634336</td>\n",
              "      <td>-0.588648</td>\n",
              "      <td>0.586828</td>\n",
              "      <td>0.418703</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>0.601789</td>\n",
              "      <td>-0.030000</td>\n",
              "      <td>0.135939</td>\n",
              "      <td>0.667244</td>\n",
              "      <td>-0.372270</td>\n",
              "      <td>-0.299436</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>-0.111177</td>\n",
              "      <td>-0.004152</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>-0.006488</td>\n",
              "      <td>-0.036557</td>\n",
              "      <td>0.040732</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>-0.491234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>-0.226757</td>\n",
              "      <td>0.191321</td>\n",
              "      <td>-0.263018</td>\n",
              "      <td>0.199013</td>\n",
              "      <td>-0.314975</td>\n",
              "      <td>-0.604491</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.208557</td>\n",
              "      <td>0.273958</td>\n",
              "      <td>0.101690</td>\n",
              "      <td>-0.230638</td>\n",
              "      <td>0.253272</td>\n",
              "      <td>0.131354</td>\n",
              "      <td>0.056416</td>\n",
              "      <td>-0.463649</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.322639</td>\n",
              "      <td>0.040448</td>\n",
              "      <td>0.298398</td>\n",
              "      <td>0.046017</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.144294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>0.548973</td>\n",
              "      <td>-0.585095</td>\n",
              "      <td>0.648615</td>\n",
              "      <td>-0.676648</td>\n",
              "      <td>0.479614</td>\n",
              "      <td>0.432241</td>\n",
              "      <td>0.601789</td>\n",
              "      <td>-0.208557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.006167</td>\n",
              "      <td>0.114613</td>\n",
              "      <td>0.573957</td>\n",
              "      <td>-0.294432</td>\n",
              "      <td>-0.328975</td>\n",
              "      <td>0.131001</td>\n",
              "      <td>-0.042371</td>\n",
              "      <td>0.044006</td>\n",
              "      <td>-0.034885</td>\n",
              "      <td>-0.131000</td>\n",
              "      <td>-0.111842</td>\n",
              "      <td>0.069959</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.585522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>-0.020567</td>\n",
              "      <td>0.012801</td>\n",
              "      <td>-0.019272</td>\n",
              "      <td>0.011519</td>\n",
              "      <td>-0.115317</td>\n",
              "      <td>-0.117637</td>\n",
              "      <td>-0.030000</td>\n",
              "      <td>0.273958</td>\n",
              "      <td>-0.006167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.016044</td>\n",
              "      <td>-0.082089</td>\n",
              "      <td>0.167676</td>\n",
              "      <td>-0.007267</td>\n",
              "      <td>0.025529</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>0.060097</td>\n",
              "      <td>0.003580</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.058483</td>\n",
              "      <td>-0.101488</td>\n",
              "      <td>-0.002718</td>\n",
              "      <td>-0.071105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>0.046002</td>\n",
              "      <td>-0.161325</td>\n",
              "      <td>0.098516</td>\n",
              "      <td>-0.098627</td>\n",
              "      <td>0.058686</td>\n",
              "      <td>-0.023851</td>\n",
              "      <td>0.135939</td>\n",
              "      <td>0.101690</td>\n",
              "      <td>0.114613</td>\n",
              "      <td>-0.016044</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.185638</td>\n",
              "      <td>-0.135423</td>\n",
              "      <td>0.171719</td>\n",
              "      <td>-0.074832</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>-0.027579</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>0.116106</td>\n",
              "      <td>0.100293</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>-0.037948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>0.621884</td>\n",
              "      <td>-0.534392</td>\n",
              "      <td>0.614504</td>\n",
              "      <td>-0.593948</td>\n",
              "      <td>0.596898</td>\n",
              "      <td>0.415834</td>\n",
              "      <td>0.667244</td>\n",
              "      <td>-0.230638</td>\n",
              "      <td>0.573957</td>\n",
              "      <td>-0.082089</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.585409</td>\n",
              "      <td>-0.223431</td>\n",
              "      <td>-0.117591</td>\n",
              "      <td>-0.057100</td>\n",
              "      <td>-0.023511</td>\n",
              "      <td>0.062484</td>\n",
              "      <td>-0.056184</td>\n",
              "      <td>-0.191742</td>\n",
              "      <td>-0.022328</td>\n",
              "      <td>-0.000479</td>\n",
              "      <td>-0.573511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>-0.377803</td>\n",
              "      <td>0.208821</td>\n",
              "      <td>-0.314396</td>\n",
              "      <td>0.269842</td>\n",
              "      <td>-0.438118</td>\n",
              "      <td>-0.235623</td>\n",
              "      <td>-0.372270</td>\n",
              "      <td>0.253272</td>\n",
              "      <td>-0.294432</td>\n",
              "      <td>0.167676</td>\n",
              "      <td>0.185638</td>\n",
              "      <td>-0.585409</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.071641</td>\n",
              "      <td>0.136080</td>\n",
              "      <td>-0.001529</td>\n",
              "      <td>0.110751</td>\n",
              "      <td>-0.174328</td>\n",
              "      <td>0.041421</td>\n",
              "      <td>0.123266</td>\n",
              "      <td>-0.024368</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>0.244081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>-0.219164</td>\n",
              "      <td>0.263707</td>\n",
              "      <td>-0.253805</td>\n",
              "      <td>0.257236</td>\n",
              "      <td>-0.246694</td>\n",
              "      <td>-0.188360</td>\n",
              "      <td>-0.299436</td>\n",
              "      <td>0.131354</td>\n",
              "      <td>-0.328975</td>\n",
              "      <td>-0.007267</td>\n",
              "      <td>-0.135423</td>\n",
              "      <td>-0.223431</td>\n",
              "      <td>0.071641</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.529918</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>-0.020316</td>\n",
              "      <td>0.030478</td>\n",
              "      <td>0.007677</td>\n",
              "      <td>-0.055183</td>\n",
              "      <td>-0.035727</td>\n",
              "      <td>-0.001405</td>\n",
              "      <td>0.179851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>-0.034669</td>\n",
              "      <td>-0.013570</td>\n",
              "      <td>-0.021710</td>\n",
              "      <td>-0.013093</td>\n",
              "      <td>0.034147</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>0.056416</td>\n",
              "      <td>0.131001</td>\n",
              "      <td>0.025529</td>\n",
              "      <td>0.171719</td>\n",
              "      <td>-0.117591</td>\n",
              "      <td>0.136080</td>\n",
              "      <td>-0.529918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.096587</td>\n",
              "      <td>-0.059190</td>\n",
              "      <td>0.146164</td>\n",
              "      <td>0.070050</td>\n",
              "      <td>0.373256</td>\n",
              "      <td>0.326677</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.109640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>-0.068917</td>\n",
              "      <td>0.151906</td>\n",
              "      <td>-0.058884</td>\n",
              "      <td>0.043266</td>\n",
              "      <td>-0.113919</td>\n",
              "      <td>0.308598</td>\n",
              "      <td>-0.111177</td>\n",
              "      <td>-0.463649</td>\n",
              "      <td>-0.042371</td>\n",
              "      <td>-0.123520</td>\n",
              "      <td>-0.074832</td>\n",
              "      <td>-0.057100</td>\n",
              "      <td>-0.001529</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>0.096587</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.051181</td>\n",
              "      <td>-0.040882</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>-0.151698</td>\n",
              "      <td>0.028059</td>\n",
              "      <td>-0.001981</td>\n",
              "      <td>0.010255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>-0.014651</td>\n",
              "      <td>-0.027515</td>\n",
              "      <td>0.076460</td>\n",
              "      <td>-0.102508</td>\n",
              "      <td>-0.083243</td>\n",
              "      <td>-0.005237</td>\n",
              "      <td>-0.004152</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.044006</td>\n",
              "      <td>0.060097</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>-0.023511</td>\n",
              "      <td>0.110751</td>\n",
              "      <td>-0.020316</td>\n",
              "      <td>-0.059190</td>\n",
              "      <td>-0.051181</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.079604</td>\n",
              "      <td>-0.113362</td>\n",
              "      <td>-0.194899</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>-0.000846</td>\n",
              "      <td>-0.130107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>-0.008508</td>\n",
              "      <td>0.132443</td>\n",
              "      <td>-0.076332</td>\n",
              "      <td>0.029402</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>-0.195340</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.322639</td>\n",
              "      <td>-0.034885</td>\n",
              "      <td>0.003580</td>\n",
              "      <td>-0.027579</td>\n",
              "      <td>0.062484</td>\n",
              "      <td>-0.174328</td>\n",
              "      <td>0.030478</td>\n",
              "      <td>0.146164</td>\n",
              "      <td>-0.040882</td>\n",
              "      <td>-0.079604</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.057546</td>\n",
              "      <td>0.215653</td>\n",
              "      <td>0.176058</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>0.061847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>0.009281</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>-0.052056</td>\n",
              "      <td>0.136679</td>\n",
              "      <td>0.047771</td>\n",
              "      <td>-0.067605</td>\n",
              "      <td>-0.006488</td>\n",
              "      <td>0.040448</td>\n",
              "      <td>-0.131000</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>-0.056184</td>\n",
              "      <td>0.041421</td>\n",
              "      <td>0.007677</td>\n",
              "      <td>0.070050</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>-0.113362</td>\n",
              "      <td>0.057546</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.193977</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>0.071052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>-0.122772</td>\n",
              "      <td>0.053835</td>\n",
              "      <td>-0.190582</td>\n",
              "      <td>0.188036</td>\n",
              "      <td>-0.043759</td>\n",
              "      <td>-0.260783</td>\n",
              "      <td>-0.036557</td>\n",
              "      <td>0.298398</td>\n",
              "      <td>-0.111842</td>\n",
              "      <td>0.058483</td>\n",
              "      <td>0.116106</td>\n",
              "      <td>-0.191742</td>\n",
              "      <td>0.123266</td>\n",
              "      <td>-0.055183</td>\n",
              "      <td>0.373256</td>\n",
              "      <td>-0.151698</td>\n",
              "      <td>-0.194899</td>\n",
              "      <td>0.215653</td>\n",
              "      <td>0.193977</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.214002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>0.070111</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>0.005346</td>\n",
              "      <td>-0.011316</td>\n",
              "      <td>0.108422</td>\n",
              "      <td>-0.065641</td>\n",
              "      <td>0.040732</td>\n",
              "      <td>0.046017</td>\n",
              "      <td>0.069959</td>\n",
              "      <td>-0.101488</td>\n",
              "      <td>0.100293</td>\n",
              "      <td>-0.022328</td>\n",
              "      <td>-0.024368</td>\n",
              "      <td>-0.035727</td>\n",
              "      <td>0.326677</td>\n",
              "      <td>0.028059</td>\n",
              "      <td>-0.045189</td>\n",
              "      <td>0.176058</td>\n",
              "      <td>0.036830</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.001503</td>\n",
              "      <td>0.102024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>-0.001280</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>-0.002001</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>-0.002718</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>-0.000479</td>\n",
              "      <td>-0.000400</td>\n",
              "      <td>-0.001405</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>-0.001981</td>\n",
              "      <td>-0.000846</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>-0.001503</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <td>-0.505761</td>\n",
              "      <td>0.491878</td>\n",
              "      <td>-0.682095</td>\n",
              "      <td>0.735981</td>\n",
              "      <td>-0.338639</td>\n",
              "      <td>-0.435088</td>\n",
              "      <td>-0.491234</td>\n",
              "      <td>0.144294</td>\n",
              "      <td>-0.585522</td>\n",
              "      <td>-0.071105</td>\n",
              "      <td>-0.037948</td>\n",
              "      <td>-0.573511</td>\n",
              "      <td>0.244081</td>\n",
              "      <td>0.179851</td>\n",
              "      <td>0.109640</td>\n",
              "      <td>0.010255</td>\n",
              "      <td>-0.130107</td>\n",
              "      <td>0.061847</td>\n",
              "      <td>0.071052</td>\n",
              "      <td>0.214002</td>\n",
              "      <td>0.102024</td>\n",
              "      <td>0.002261</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0079fea1-f224-483c-92ec-0f00ff7b2311')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0079fea1-f224-483c-92ec-0f00ff7b2311 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0079fea1-f224-483c-92ec-0f00ff7b2311');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13ec8760-6a2c-457c-8785-13677775de08\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13ec8760-6a2c-457c-8785-13677775de08')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13ec8760-6a2c-457c-8785-13677775de08 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Model on 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V13', 'V15',\n",
        "       'V16', 'V19', 'V20', 'V21', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\n",
        "       'Amount'"
      ],
      "metadata": {
        "id": "rqjaTkLRYgIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying a filter for columns with less co-relation**"
      ],
      "metadata": {
        "id": "ZEhRBDZfmG_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_f=filtered_df.drop(filtered_df.columns[abs(filtered_df.corrwith(filtered_df['Class'])) < 0.2], axis=1)"
      ],
      "metadata": {
        "id": "d0oAtR-cbUnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1H4VoYukXmDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection"
      ],
      "metadata": {
        "id": "yjZrexYEJG32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "mWL-8wBnqQUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First Logistic Regression Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bUQN2mjubSsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9anaL9I2bjoA",
        "outputId": "42a81fb3-d9e6-49d3-86e0-b516e090bacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V9', 'V16', 'V19', 'V27',\n",
              "       'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_f.drop('Class',axis=1).values"
      ],
      "metadata": {
        "id": "ugsLEIYVbmtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=df_f.Class.values"
      ],
      "metadata": {
        "id": "9FtunlIYcQce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "7mDxPWwRcUvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scale=StandardScaler()\n",
        "x_train=scale.fit_transform(x_train)\n",
        "x_test=scale.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "c3KllPMvcaif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "modelLogi=LogisticRegression()\n",
        "modelLogi.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "E7u7ZGbMcbYG",
        "outputId": "61cc0cbd-9891-4156-e2cc-a6689afd0a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predLogi=modelLogi.predict(x_test)\n",
        "predLogi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdd58WEBcfVo",
        "outputId": "0b1e95ee-410b-4518-a957-ce788b3adf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
      ],
      "metadata": {
        "id": "M-zkJSpsJgpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_logi=confusion_matrix(y_test,predLogi)\n",
        "acc_logi=accuracy_score(y_test,predLogi)"
      ],
      "metadata": {
        "id": "P08zSvPdcp74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_logi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19-cPQZics8P",
        "outputId": "6ba2813b-18c5-47c3-d74d-88c6e30de0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.938758283037184"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_logi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds04fcvecvW4",
        "outputId": "a28bd8a8-30ff-4846-f3bb-7d2ed555e195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[67528,  3328],\n",
              "       [ 5378, 65924]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_qqjMbBdCcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Data Seems to be over fitted for the above model\n",
        "\n",
        "Applying some changes to features"
      ],
      "metadata": {
        "id": "JwhWmjgmkGI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Changing the threshold for feature selection"
      ],
      "metadata": {
        "id": "_YkMUyoznH4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###second Logistic Regression Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uYx2bDF3mnxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop('Class', axis=1)\n",
        "\n",
        "correlation_matrix = features.corr()\n",
        "\n",
        "correlation_threshold = 0.6\n",
        "\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "print(\"Highly Correlated Features:\", highly_correlated_features)\n",
        "\n",
        "features_filtered = features.drop(highly_correlated_features, axis=1)\n",
        "filtered_df = pd.concat([features_filtered, df['Class']], axis=1)\n",
        "\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(filtered_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SynzLEqHlJGS",
        "outputId": "1976ab84-1bdd-44b8-fb59-fd9dcd4c72b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly Correlated Features: {'V3', 'V18', 'V12', 'V10', 'V11', 'V17', 'V7', 'V19', 'V9', 'V4', 'V22', 'V5', 'V14', 'V16', 'V8'}\n",
            "Filtered DataFrame:\n",
            "   id        V1        V2        V6       V13       V15       V20       V21  \\\n",
            "0   0 -0.260648 -0.469648  0.732898 -0.941386  1.804879  0.091202 -0.110552   \n",
            "1   1  0.985100 -0.356045  0.428605  0.574074  0.706121 -0.233984 -0.194936   \n",
            "2   2 -0.260272 -0.949385  1.419481  0.805173  3.069025  0.361652 -0.005020   \n",
            "3   3 -0.152152 -0.508959  1.143312  0.592994 -0.697664 -0.378223 -0.146927   \n",
            "4   4 -0.206820 -0.165280  0.530549  1.439310  0.153008  0.247237 -0.106984   \n",
            "\n",
            "        V23       V24       V25       V26       V27       V28    Amount  Class  \n",
            "0 -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
            "1  0.079469 -0.577395  0.190090  0.296503 -0.248052 -0.064512   6531.37      0  \n",
            "2  0.945045 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
            "3 -0.214048 -1.893131  1.003963 -0.515950 -0.165316  0.048424   5384.44      0  \n",
            "4 -0.161666  0.312561 -0.414116  1.071126  0.023712  0.419117  14278.97      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgXjoKyXhQkj",
        "outputId": "f71dc9d9-069a-4647-d39b-fabb13ea2e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'V1', 'V2', 'V6', 'V13', 'V15', 'V20', 'V21', 'V23', 'V24', 'V25',\n",
              "       'V26', 'V27', 'V28', 'Amount', 'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=filtered_df.drop(['Class','id'],axis=1).values\n",
        "Y=filtered_df.Class.values"
      ],
      "metadata": {
        "id": "QAg3jV5YnR1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0RuwdrEojQ1",
        "outputId": "487ddc46-ab9b-4b66-ad55-872093c8ea45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8359741835640047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vud5hzxQorYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree"
      ],
      "metadata": {
        "id": "IYIMrKykqXRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First Decision Tree Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yBYGacFwmw5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop('Class', axis=1)\n",
        "\n",
        "correlation_matrix = features.corr()\n",
        "\n",
        "correlation_threshold = 0.5\n",
        "\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "print(\"Highly Correlated Features:\", highly_correlated_features)\n",
        "\n",
        "features_filtered = features.drop(highly_correlated_features, axis=1)\n",
        "filtered_df = pd.concat([features_filtered, df['Class']], axis=1)\n",
        "\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(filtered_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWFDsZR2rQc7",
        "outputId": "f2f78deb-9de1-4f74-d12e-55f0005038b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly Correlated Features: {'V3', 'V18', 'V12', 'V10', 'V11', 'V17', 'V7', 'V19', 'V6', 'V9', 'V4', 'V22', 'V2', 'V5', 'V14', 'V16', 'V21', 'V8'}\n",
            "Filtered DataFrame:\n",
            "   id        V1       V13       V15       V20       V23       V24       V25  \\\n",
            "0   0 -0.260648 -0.941386  1.804879  0.091202 -0.134794  0.165959  0.126280   \n",
            "1   1  0.985100  0.574074  0.706121 -0.233984  0.079469 -0.577395  0.190090   \n",
            "2   2 -0.260272  0.805173  3.069025  0.361652  0.945045 -1.154666 -0.605564   \n",
            "3   3 -0.152152  0.592994 -0.697664 -0.378223 -0.214048 -1.893131  1.003963   \n",
            "4   4 -0.206820  1.439310  0.153008  0.247237 -0.161666  0.312561 -0.414116   \n",
            "\n",
            "        V26       V27       V28    Amount  Class  \n",
            "0 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
            "1  0.296503 -0.248052 -0.064512   6531.37      0  \n",
            "2 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
            "3 -0.515950 -0.165316  0.048424   5384.44      0  \n",
            "4  1.071126  0.023712  0.419117  14278.97      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "dQNC9B2guUXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxbVRT5ki-t9",
        "outputId": "9b873d01-008a-473d-e0b4-e9baa1eb0c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V13', 'V15', 'V20', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\n",
              "       'Amount', 'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=filtered_df.drop('Class',axis=1).values\n",
        "Y=filtered_df.Class.values\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=0)\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "model = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and print it\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z28hH9Q8qaTm",
        "outputId": "9ba742c8-42aa-403d-c7c5-eeaf89ce10c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Seems to be over fitted for the above model\n",
        "\n",
        "Applying some changes to features\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u1HyHqmx_W_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second Decision Tree Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SCIVmCIDm3Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop('Class', axis=1)\n",
        "\n",
        "correlation_matrix = features.corr()\n",
        "#changing threshold to 0.6\n",
        "correlation_threshold = 0.6\n",
        "\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "print(\"Highly Correlated Features:\", highly_correlated_features)\n",
        "\n",
        "features_filtered = features.drop(highly_correlated_features, axis=1)\n",
        "filtered_df = pd.concat([features_filtered, df['Class']], axis=1)\n",
        "\n",
        "print(\"Filtered DataFrame:\")\n",
        "\n",
        "filtered_df.drop('id',axis=1,inplace=True)\n",
        "print(filtered_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYHEmnMFvDLC",
        "outputId": "e648bb47-a9e3-4be5-ecf5-49200591443c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly Correlated Features: {'V3', 'V18', 'V12', 'V10', 'V11', 'V17', 'V7', 'V19', 'V9', 'V4', 'V22', 'V5', 'V14', 'V16', 'V8'}\n",
            "Filtered DataFrame:\n",
            "         V1        V2        V6       V13       V15       V20       V21  \\\n",
            "0 -0.260648 -0.469648  0.732898 -0.941386  1.804879  0.091202 -0.110552   \n",
            "1  0.985100 -0.356045  0.428605  0.574074  0.706121 -0.233984 -0.194936   \n",
            "2 -0.260272 -0.949385  1.419481  0.805173  3.069025  0.361652 -0.005020   \n",
            "3 -0.152152 -0.508959  1.143312  0.592994 -0.697664 -0.378223 -0.146927   \n",
            "4 -0.206820 -0.165280  0.530549  1.439310  0.153008  0.247237 -0.106984   \n",
            "\n",
            "        V23       V24       V25       V26       V27       V28    Amount  Class  \n",
            "0 -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
            "1  0.079469 -0.577395  0.190090  0.296503 -0.248052 -0.064512   6531.37      0  \n",
            "2  0.945045 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
            "3 -0.214048 -1.893131  1.003963 -0.515950 -0.165316  0.048424   5384.44      0  \n",
            "4 -0.161666  0.312561 -0.414116  1.071126  0.023712  0.419117  14278.97      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_Sg73ns6yZ",
        "outputId": "ffe807ab-4297-4d53-b5d3-ab21ea82de8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V6', 'V13', 'V15', 'V20', 'V21', 'V23', 'V24', 'V25',\n",
              "       'V26', 'V27', 'V28', 'Amount', 'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_f=filtered_df.drop(filtered_df.columns[abs(filtered_df.corrwith(filtered_df['Class'])) < 0.2], axis=1)"
      ],
      "metadata": {
        "id": "-pl_4ND9q6es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing features that are having less corelation to the target than 0.2 %"
      ],
      "metadata": {
        "id": "NqG_4NiYAL7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkgXFXqJsyPr",
        "outputId": "8f550485-87cc-4904-8e73-b10f9d68cd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V6', 'V27', 'Class'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "# Initialize the Decision Tree model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and print it\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS7nNMLosNtH",
        "outputId": "652ef8fa-54d0-4eb1-d9f7-51990bc32abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKRyh7LHsoEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##Random Forest"
      ],
      "metadata": {
        "id": "nvundnKdw57F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random forest model Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pWNil09Jm7le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "# Initialize the Decision Tree model\n",
        "model = RandomForestClassifier(n_estimators=100,random_state=1)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and print it\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZtahbzzw8ch",
        "outputId": "fdff4ce8-cc3f-4c62-f528-691e4d9372d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "xn5P1UiqnHin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest Model with Random search\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVgUC65jnA9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "subsample_size = 5000\n",
        "X_train_subsample, _, y_train_subsample, _ = train_test_split(X_train, y_train, train_size=subsample_size, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_subsample_scaled = scaler.fit_transform(X_train_subsample)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters and their possible values\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf_model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "random_search.fit(X_train_subsample_scaled, y_train_subsample)\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "\n",
        "y_pred = random_search.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-dFKCiwOyzG",
        "outputId": "d37caed5-4488-461c-a1ca-be53be36c0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None}\n",
            "Accuracy: 0.8998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest Model with Grid Search\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xkf0diXanVwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "subsample_size = 5000\n",
        "X_train_subsample, _, y_train_subsample, _ = train_test_split(X_train, y_train, train_size=subsample_size, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_subsample_scaled = scaler.fit_transform(X_train_subsample)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters and their possible values\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    rf_model, param_grid=param_grid, scoring='accuracy', cv=3\n",
        ")\n",
        "\n",
        "\n",
        "grid_search.fit(X_train_subsample_scaled, y_train_subsample)\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuG3sEnPxg7e",
        "outputId": "ce4d26a5-c785-4861-8499-978b8807263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Accuracy: 0.9035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "##SVM\n"
      ],
      "metadata": {
        "id": "Hf6A5VgizFsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "\n",
        "model = SVC(kernel='rbf', random_state=1)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeCZCfF6zVSm",
        "outputId": "1360e293-a1fe-459e-db7b-3569f5fa8011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GradientBoosting"
      ],
      "metadata": {
        "id": "v74xOB-O5Lbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_f.drop('Class',axis=1).values\n",
        "Y=df_f.Class.values\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=1)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4UCtOF8z_jc",
        "outputId": "f0c394d5-94db-4fc7-fb4e-5e97944b3112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ANN"
      ],
      "metadata": {
        "id": "nM-zOEZFA1fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop(['Class','id'],axis=1).values\n",
        "Y=df_f.Class.values\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming X and y are your features and labels\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Standardize the data using StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "#Adding input layer\n",
        "ann.add(tf.keras.layers.Dense(units = 9 , activation='relu'))\n",
        "#Adding second hidden line\n",
        "ann.add(tf.keras.layers.Dense(units=3,activation='relu'))\n",
        "#compile the ann\n",
        "\n",
        "ann.compile(optimizer = 'adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "ann.fit(x_train, y_train,batch_size=32,epochs=50)\n",
        "\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy and print it\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB579BUw5jlF",
        "outputId": "d558bd6f-eab1-4819-968f-7233e6c15147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12439/12439 [==============================] - 28s 2ms/step - loss: 0.3946 - accuracy: 0.4864\n",
            "Epoch 2/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.1082 - accuracy: 0.4335\n",
            "Epoch 3/50\n",
            "12439/12439 [==============================] - 28s 2ms/step - loss: 0.0723 - accuracy: 0.4603\n",
            "Epoch 4/50\n",
            "12439/12439 [==============================] - 39s 3ms/step - loss: 0.0702 - accuracy: 0.5155\n",
            "Epoch 5/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0570 - accuracy: 0.5213\n",
            "Epoch 6/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.0585 - accuracy: 0.5293\n",
            "Epoch 7/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0631 - accuracy: 0.4954\n",
            "Epoch 8/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0550 - accuracy: 0.4962\n",
            "Epoch 9/50\n",
            "12439/12439 [==============================] - 36s 3ms/step - loss: 0.0624 - accuracy: 0.5440\n",
            "Epoch 10/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.0490 - accuracy: 0.5910\n",
            "Epoch 11/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0570 - accuracy: 0.5597\n",
            "Epoch 12/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0461 - accuracy: 0.6282\n",
            "Epoch 13/50\n",
            "12439/12439 [==============================] - 35s 3ms/step - loss: 0.0453 - accuracy: 0.5804\n",
            "Epoch 14/50\n",
            "12439/12439 [==============================] - 36s 3ms/step - loss: 0.0449 - accuracy: 0.5019\n",
            "Epoch 15/50\n",
            "12439/12439 [==============================] - 29s 2ms/step - loss: 0.0463 - accuracy: 0.5062\n",
            "Epoch 16/50\n",
            "12439/12439 [==============================] - 33s 3ms/step - loss: 0.0499 - accuracy: 0.6772\n",
            "Epoch 17/50\n",
            "12439/12439 [==============================] - 30s 2ms/step - loss: 0.0463 - accuracy: 0.7200\n",
            "Epoch 18/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0481 - accuracy: 0.5524\n",
            "Epoch 19/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0473 - accuracy: 0.5374\n",
            "Epoch 20/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0471 - accuracy: 0.5083\n",
            "Epoch 21/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0470 - accuracy: 0.4970\n",
            "Epoch 22/50\n",
            "12439/12439 [==============================] - 33s 3ms/step - loss: 0.0401 - accuracy: 0.5845\n",
            "Epoch 23/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0431 - accuracy: 0.5136\n",
            "Epoch 24/50\n",
            "12439/12439 [==============================] - 31s 3ms/step - loss: 0.0412 - accuracy: 0.7118\n",
            "Epoch 25/50\n",
            "12439/12439 [==============================] - 32s 3ms/step - loss: 0.0394 - accuracy: 0.6924\n",
            "Epoch 26/50\n",
            "12439/12439 [==============================] - 33s 3ms/step - loss: 0.0417 - accuracy: 0.8573\n",
            "Epoch 27/50\n",
            "12439/12439 [==============================] - 33s 3ms/step - loss: 0.0436 - accuracy: 0.8473\n",
            "Epoch 28/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0376 - accuracy: 0.8560\n",
            "Epoch 29/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.0372 - accuracy: 0.9440\n",
            "Epoch 30/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0377 - accuracy: 0.9410\n",
            "Epoch 31/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0413 - accuracy: 0.9459\n",
            "Epoch 32/50\n",
            "12439/12439 [==============================] - 37s 3ms/step - loss: 0.0372 - accuracy: 0.9344\n",
            "Epoch 33/50\n",
            "12439/12439 [==============================] - 31s 2ms/step - loss: 0.0426 - accuracy: 0.9440\n",
            "Epoch 34/50\n",
            "12439/12439 [==============================] - 31s 2ms/step - loss: 0.0396 - accuracy: 0.9389\n",
            "Epoch 35/50\n",
            "12439/12439 [==============================] - 40s 3ms/step - loss: 0.0469 - accuracy: 0.9514\n",
            "Epoch 36/50\n",
            "12439/12439 [==============================] - 29s 2ms/step - loss: 0.0445 - accuracy: 0.9269\n",
            "Epoch 37/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0374 - accuracy: 0.9086\n",
            "Epoch 38/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0387 - accuracy: 0.7796\n",
            "Epoch 39/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0443 - accuracy: 0.8517\n",
            "Epoch 40/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0365 - accuracy: 0.9013\n",
            "Epoch 41/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.0377 - accuracy: 0.8762\n",
            "Epoch 42/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0350 - accuracy: 0.8061\n",
            "Epoch 43/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0378 - accuracy: 0.6683\n",
            "Epoch 44/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0342 - accuracy: 0.6886\n",
            "Epoch 45/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0373 - accuracy: 0.7201\n",
            "Epoch 46/50\n",
            "12439/12439 [==============================] - 24s 2ms/step - loss: 0.0398 - accuracy: 0.5332\n",
            "Epoch 47/50\n",
            "12439/12439 [==============================] - 27s 2ms/step - loss: 0.0366 - accuracy: 0.5053\n",
            "Epoch 48/50\n",
            "12439/12439 [==============================] - 25s 2ms/step - loss: 0.0395 - accuracy: 0.5127\n",
            "Epoch 49/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0397 - accuracy: 0.5020\n",
            "Epoch 50/50\n",
            "12439/12439 [==============================] - 26s 2ms/step - loss: 0.0379 - accuracy: 0.5033\n",
            "Accuracy: 0.8922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy Table"
      ],
      "metadata": {
        "id": "-Qi01rncEsgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`*Here first filter is applied to remove the multiple co-linearity btn the columns`\n",
        "\n",
        "`*2nd one is to remove less co-related columns to the target column`"
      ],
      "metadata": {
        "id": "jA2C1J9aIBbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_data = {\n",
        "    'Model': ['Logistic Regression 1','Logistic Regression 2','Decision Tree 1','Decision Tree 2','Random Forest','*Random Forest Hypertuned(Random Search)','*Random Forest Hypertuned(Grid Search)','SVM','GradientBoosting','ANN'],\n",
        "    'Accuracy%': [0.938, 0.8359,  0.9890, 0.9526, 0.9724,0.8998,0.9010,0.8914,0.8916,0.8922],\n",
        "    'Filter1':[0.7,0.6,0.5,0.6,0.6,0.6,0.6,0.6,0.6,0.6],\n",
        "    'Filter2':[0.2,0,0,0.2,0.2,0.2,0.2,0.2,0.2,0.2]\n",
        "}\n",
        "\n",
        "accuracy_df = pd.DataFrame(accuracy_data)\n",
        "accuracy_df['Accuracy%'] = accuracy_df['Accuracy%'] * 100\n",
        "accuracy_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ymn849Mi9KfU",
        "outputId": "48143e00-f783-45f7-d3d4-3613bc42cb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      Model  Accuracy%  Filter1  Filter2\n",
              "0                     Logistic Regression 1      93.80      0.7      0.2\n",
              "1                     Logistic Regression 2      83.59      0.6      0.0\n",
              "2                           Decision Tree 1      98.90      0.5      0.0\n",
              "3                           Decision Tree 2      95.26      0.6      0.2\n",
              "4                             Random Forest      97.24      0.6      0.2\n",
              "5  *Random Forest Hypertuned(Random Search)      89.98      0.6      0.2\n",
              "6    *Random Forest Hypertuned(Grid Search)      90.10      0.6      0.2\n",
              "7                                       SVM      89.14      0.6      0.2\n",
              "8                          GradientBoosting      89.16      0.6      0.2\n",
              "9                                       ANN      89.22      0.6      0.2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-356797ea-482c-4c41-9511-200f1ead0952\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy%</th>\n",
              "      <th>Filter1</th>\n",
              "      <th>Filter2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression 1</td>\n",
              "      <td>93.80</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression 2</td>\n",
              "      <td>83.59</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree 1</td>\n",
              "      <td>98.90</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decision Tree 2</td>\n",
              "      <td>95.26</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>97.24</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>*Random Forest Hypertuned(Random Search)</td>\n",
              "      <td>89.98</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>*Random Forest Hypertuned(Grid Search)</td>\n",
              "      <td>90.10</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SVM</td>\n",
              "      <td>89.14</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GradientBoosting</td>\n",
              "      <td>89.16</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ANN</td>\n",
              "      <td>89.22</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-356797ea-482c-4c41-9511-200f1ead0952')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-356797ea-482c-4c41-9511-200f1ead0952 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-356797ea-482c-4c41-9511-200f1ead0952');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ebbfceae-7477-4153-9e12-bf9e1fd9bd98\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebbfceae-7477-4153-9e12-bf9e1fd9bd98')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ebbfceae-7477-4153-9e12-bf9e1fd9bd98 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6fa21cc8-9fec-4869-8f94-ee3ef89be9bf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('accuracy_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6fa21cc8-9fec-4869-8f94-ee3ef89be9bf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('accuracy_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZi9bd9EiPUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Model': ['Logistic Regression 1', 'Logistic Regression 2', 'Decision Tree 1', 'Decision Tree 2', 'Random Forest', 'Random Forest Hypertuned(Random Search)', 'Random Forest Hypertuned(Grid Search)', 'SVM', 'GradientBoosting', 'ANN'],\n",
        "    'Accuracy': [93.8, 83.59, 0.989, 0.9526, 0.9724, 0.8998, 0.901, 0.8914, 0.8916, 0.8922],\n",
        "    'Filter1': [0.7, 0.6, 0.5, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n",
        "    'Filter2': [0.2, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
        "    'Features_used': [['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V9', 'V16', 'V19', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V13', 'V15', 'V20', 'V21', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
        "                      ['V1', 'V13', 'V15', 'V20', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27'],\n",
        "                      ['V1', 'V2', 'V6', 'V27']]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "qF_OlHfUkkM3",
        "outputId": "cfadfe78-69c4-42b3-bce9-fa7f5dfd8a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Model  Accuracy  Filter1  Filter2  \\\n",
              "0                    Logistic Regression 1   93.8000      0.7      0.2   \n",
              "1                    Logistic Regression 2   83.5900      0.6      0.0   \n",
              "2                          Decision Tree 1    0.9890      0.5      0.0   \n",
              "3                          Decision Tree 2    0.9526      0.6      0.2   \n",
              "4                            Random Forest    0.9724      0.6      0.2   \n",
              "5  Random Forest Hypertuned(Random Search)    0.8998      0.6      0.2   \n",
              "6    Random Forest Hypertuned(Grid Search)    0.9010      0.6      0.2   \n",
              "7                                      SVM    0.8914      0.6      0.2   \n",
              "8                         GradientBoosting    0.8916      0.6      0.2   \n",
              "9                                      ANN    0.8922      0.6      0.2   \n",
              "\n",
              "                                       Features_used  \n",
              "0    [V1, V2, V3, V4, V5, V6, V7, V9, V16, V19, V27]  \n",
              "1  [V1, V2, V6, V13, V15, V20, V21, V23, V24, V25...  \n",
              "2  [V1, V13, V15, V20, V23, V24, V25, V26, V27, V...  \n",
              "3                                  [V1, V2, V6, V27]  \n",
              "4                                  [V1, V2, V6, V27]  \n",
              "5                                  [V1, V2, V6, V27]  \n",
              "6                                  [V1, V2, V6, V27]  \n",
              "7                                  [V1, V2, V6, V27]  \n",
              "8                                  [V1, V2, V6, V27]  \n",
              "9                                  [V1, V2, V6, V27]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f76679dd-12d8-4fa9-b9fa-aeb81f8fc6be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Filter1</th>\n",
              "      <th>Filter2</th>\n",
              "      <th>Features_used</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression 1</td>\n",
              "      <td>93.8000</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V3, V4, V5, V6, V7, V9, V16, V19, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression 2</td>\n",
              "      <td>83.5900</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[V1, V2, V6, V13, V15, V20, V21, V23, V24, V25...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree 1</td>\n",
              "      <td>0.9890</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[V1, V13, V15, V20, V23, V24, V25, V26, V27, V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decision Tree 2</td>\n",
              "      <td>0.9526</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.9724</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Random Forest Hypertuned(Random Search)</td>\n",
              "      <td>0.8998</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Random Forest Hypertuned(Grid Search)</td>\n",
              "      <td>0.9010</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.8914</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GradientBoosting</td>\n",
              "      <td>0.8916</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ANN</td>\n",
              "      <td>0.8922</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>[V1, V2, V6, V27]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f76679dd-12d8-4fa9-b9fa-aeb81f8fc6be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f76679dd-12d8-4fa9-b9fa-aeb81f8fc6be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f76679dd-12d8-4fa9-b9fa-aeb81f8fc6be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-afae1d33-60aa-4ae3-9ee2-f8dad6af7623\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afae1d33-60aa-4ae3-9ee2-f8dad6af7623')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-afae1d33-60aa-4ae3-9ee2-f8dad6af7623 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fb911564-7008-40f1-9125-7305adbb7541\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fb911564-7008-40f1-9125-7305adbb7541 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MPlLV2ran2k6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "The dataset is balanced, as confirmed by the count plot.\n",
        "\n",
        "and number of fraudulent and  non-fraudulent is same 50-50%\n",
        "\n",
        "**Feature selection**\n",
        "\n",
        "Filter 1 - Addressing Multicollinearity:\n",
        "\n",
        "The first filter aims to eliminate multicollinearity among the columns.\n",
        "Multicollinearity occurs when two or more features in the dataset are highly correlated, leading to redundancy in the information they provide.\n",
        "\n",
        "Filter 2 - Removing Less Correlated Columns to the Target:\n",
        "\n",
        "The second filter targets columns that are less correlated with the target variable (output or label).\n",
        "Features with low correlation to the target may contribute less valuable information to the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "FACGoqv_VYEr"
      }
    }
  ]
}